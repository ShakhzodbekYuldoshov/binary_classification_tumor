{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d75a3f4f",
   "metadata": {},
   "source": [
    "### import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a55756f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.metrics import AUC, BinaryAccuracy,  TruePositives, TrueNegatives, Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6351f2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since model is trained on imagenet size should be 224x224\n",
    "IMAGE_SIZE = [224, 224]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773b1a89",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "443ce76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './dataset/train'\n",
    "test_path = './dataset/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a456bf5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2637 images belonging to 2 classes.\n",
      "Found 660 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dataget = ImageDataGenerator(\n",
    "                                    preprocessing_function=preprocess_input,\n",
    "                                    rotation_range=40,\n",
    "                                    width_shift_range=0.2,\n",
    "                                    height_shift_range=0.2,\n",
    "                                    shear_range=0.2,\n",
    "                                    zoom_range=0.2,\n",
    "                                    horizontal_flip=True,\n",
    "                                    fill_mode='nearest')\n",
    "\n",
    "test_dataget = ImageDataGenerator(\n",
    "                                    preprocessing_function=preprocess_input,\n",
    "                                    rotation_range=40,\n",
    "                                    width_shift_range=0.2,\n",
    "                                    height_shift_range=0.2,\n",
    "                                    shear_range=0.2,\n",
    "                                    zoom_range=0.2,\n",
    "                                    horizontal_flip=True,\n",
    "                                    fill_mode='nearest')\n",
    "\n",
    "train_set = train_dataget.flow_from_directory(train_path,\n",
    "                                             target_size=(224,224),\n",
    "                                             batch_size=32,\n",
    "                                             class_mode = 'categorical')\n",
    "\n",
    "test_set = test_dataget.flow_from_directory(test_path,\n",
    "                                             target_size=(224,224),\n",
    "                                             batch_size=32,\n",
    "                                             class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c91e677",
   "metadata": {},
   "source": [
    "## Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b594affb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = VGG16(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85a246bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in vgg.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fe94503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 50178     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,764,866\n",
      "Trainable params: 50,178\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "x = Flatten()(vgg.output)\n",
    "prediction = Dense(2, activation='sigmoid')(x)\n",
    "model = Model(inputs=vgg.input, outputs=prediction)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3ba08c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_keras = [\n",
    "                AUC(name=\"auc\"),\n",
    "                BinaryAccuracy(name=\"acc\"),\n",
    "                TruePositives(name=\"positive\"),\n",
    "                TrueNegatives(name=\"negative\"),\n",
    "                Precision(name=\"precision\"),\n",
    "                Recall(name=\"recall\"),\n",
    "]\n",
    "\n",
    "adam = Adam(learning_rate=0.0001)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer=adam,\n",
    "             metrics=metrics_keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6339c31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(filepath='best_model.h5',\n",
    "                              verbose=1, save_best_only=True)\n",
    "\n",
    "lr_reduce = ReduceLROnPlateau(monitor='auc', mode=\"max\", factor=0.2, min_delta=0.0001, patience=5, verbose=1)\n",
    "\n",
    "callbacks = [lr_reduce, checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f887b579",
   "metadata": {},
   "source": [
    "# train the model with 100 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38d4d324",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shahzod\\anaconda3\\envs\\keras\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 1.2743 - auc: 0.7764 - acc: 0.7152 - positive: 1864.0000 - negative: 1862.0000 - precision: 0.7150 - recall: 0.7155\n",
      "Epoch 1: val_loss improved from inf to 0.92448, saving model to best_model.h5\n",
      "82/82 [==============================] - 184s 2s/step - loss: 1.2743 - auc: 0.7764 - acc: 0.7152 - positive: 1864.0000 - negative: 1862.0000 - precision: 0.7150 - recall: 0.7155 - val_loss: 0.9245 - val_auc: 0.8567 - val_acc: 0.7862 - val_positive: 478.0000 - val_negative: 478.0000 - val_precision: 0.7862 - val_recall: 0.7862 - lr: 1.0000e-04\n",
      "Epoch 2/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.9123 - auc: 0.8559 - acc: 0.7868 - positive: 2055.0000 - negative: 2044.0000 - precision: 0.7856 - recall: 0.7889\n",
      "Epoch 2: val_loss improved from 0.92448 to 0.88811, saving model to best_model.h5\n",
      "82/82 [==============================] - 189s 2s/step - loss: 0.9123 - auc: 0.8559 - acc: 0.7868 - positive: 2055.0000 - negative: 2044.0000 - precision: 0.7856 - recall: 0.7889 - val_loss: 0.8881 - val_auc: 0.8508 - val_acc: 0.7862 - val_positive: 487.0000 - val_negative: 469.0000 - val_precision: 0.7780 - val_recall: 0.8010 - lr: 1.0000e-04\n",
      "Epoch 3/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.8220 - auc: 0.8653 - acc: 0.7894 - positive: 2058.0000 - negative: 2055.0000 - precision: 0.7891 - recall: 0.7900\n",
      "Epoch 3: val_loss improved from 0.88811 to 0.88416, saving model to best_model.h5\n",
      "82/82 [==============================] - 189s 2s/step - loss: 0.8220 - auc: 0.8653 - acc: 0.7894 - positive: 2058.0000 - negative: 2055.0000 - precision: 0.7891 - recall: 0.7900 - val_loss: 0.8842 - val_auc: 0.8662 - val_acc: 0.8026 - val_positive: 476.0000 - val_negative: 500.0000 - val_precision: 0.8151 - val_recall: 0.7829 - lr: 1.0000e-04\n",
      "Epoch 4/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.7551 - auc: 0.8810 - acc: 0.8148 - positive: 2117.0000 - negative: 2128.0000 - precision: 0.8161 - recall: 0.8127\n",
      "Epoch 4: val_loss improved from 0.88416 to 0.87138, saving model to best_model.h5\n",
      "82/82 [==============================] - 190s 2s/step - loss: 0.7551 - auc: 0.8810 - acc: 0.8148 - positive: 2117.0000 - negative: 2128.0000 - precision: 0.8161 - recall: 0.8127 - val_loss: 0.8714 - val_auc: 0.8640 - val_acc: 0.7878 - val_positive: 509.0000 - val_negative: 449.0000 - val_precision: 0.7620 - val_recall: 0.8372 - lr: 1.0000e-04\n",
      "Epoch 5/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.7227 - auc: 0.8831 - acc: 0.8092 - positive: 2109.0000 - negative: 2107.0000 - precision: 0.8090 - recall: 0.8096\n",
      "Epoch 5: val_loss improved from 0.87138 to 0.77287, saving model to best_model.h5\n",
      "82/82 [==============================] - 189s 2s/step - loss: 0.7227 - auc: 0.8831 - acc: 0.8092 - positive: 2109.0000 - negative: 2107.0000 - precision: 0.8090 - recall: 0.8096 - val_loss: 0.7729 - val_auc: 0.8740 - val_acc: 0.8010 - val_positive: 488.0000 - val_negative: 486.0000 - val_precision: 0.8000 - val_recall: 0.8026 - lr: 1.0000e-04\n",
      "Epoch 6/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.6434 - auc: 0.8971 - acc: 0.8246 - positive: 2149.0000 - negative: 2147.0000 - precision: 0.8243 - recall: 0.8250\n",
      "Epoch 6: val_loss did not improve from 0.77287\n",
      "82/82 [==============================] - 191s 2s/step - loss: 0.6434 - auc: 0.8971 - acc: 0.8246 - positive: 2149.0000 - negative: 2147.0000 - precision: 0.8243 - recall: 0.8250 - val_loss: 0.7859 - val_auc: 0.8683 - val_acc: 0.7936 - val_positive: 452.0000 - val_negative: 513.0000 - val_precision: 0.8263 - val_recall: 0.7434 - lr: 1.0000e-04\n",
      "Epoch 7/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.6406 - auc: 0.8985 - acc: 0.8267 - positive: 2144.0000 - negative: 2163.0000 - precision: 0.8291 - recall: 0.8230\n",
      "Epoch 7: val_loss improved from 0.77287 to 0.72572, saving model to best_model.h5\n",
      "82/82 [==============================] - 189s 2s/step - loss: 0.6406 - auc: 0.8985 - acc: 0.8267 - positive: 2144.0000 - negative: 2163.0000 - precision: 0.8291 - recall: 0.8230 - val_loss: 0.7257 - val_auc: 0.8934 - val_acc: 0.8232 - val_positive: 478.0000 - val_negative: 523.0000 - val_precision: 0.8490 - val_recall: 0.7862 - lr: 1.0000e-04\n",
      "Epoch 8/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.6022 - auc: 0.9074 - acc: 0.8330 - positive: 2174.0000 - negative: 2166.0000 - precision: 0.8320 - recall: 0.8345\n",
      "Epoch 8: val_loss improved from 0.72572 to 0.66965, saving model to best_model.h5\n",
      "82/82 [==============================] - 193s 2s/step - loss: 0.6022 - auc: 0.9074 - acc: 0.8330 - positive: 2174.0000 - negative: 2166.0000 - precision: 0.8320 - recall: 0.8345 - val_loss: 0.6696 - val_auc: 0.8934 - val_acc: 0.8109 - val_positive: 484.0000 - val_negative: 502.0000 - val_precision: 0.8203 - val_recall: 0.7961 - lr: 1.0000e-04\n",
      "Epoch 9/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.5797 - auc: 0.9116 - acc: 0.8347 - positive: 2174.0000 - negative: 2175.0000 - precision: 0.8349 - recall: 0.8345\n",
      "Epoch 9: val_loss did not improve from 0.66965\n",
      "82/82 [==============================] - 191s 2s/step - loss: 0.5797 - auc: 0.9116 - acc: 0.8347 - positive: 2174.0000 - negative: 2175.0000 - precision: 0.8349 - recall: 0.8345 - val_loss: 0.7556 - val_auc: 0.8769 - val_acc: 0.8076 - val_positive: 470.0000 - val_negative: 512.0000 - val_precision: 0.8304 - val_recall: 0.7730 - lr: 1.0000e-04\n",
      "Epoch 10/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.5522 - auc: 0.9139 - acc: 0.8397 - positive: 2179.0000 - negative: 2196.0000 - precision: 0.8420 - recall: 0.8365\n",
      "Epoch 10: val_loss did not improve from 0.66965\n",
      "82/82 [==============================] - 194s 2s/step - loss: 0.5522 - auc: 0.9139 - acc: 0.8397 - positive: 2179.0000 - negative: 2196.0000 - precision: 0.8420 - recall: 0.8365 - val_loss: 0.7323 - val_auc: 0.8828 - val_acc: 0.8051 - val_positive: 500.0000 - val_negative: 479.0000 - val_precision: 0.7949 - val_recall: 0.8224 - lr: 1.0000e-04\n",
      "Epoch 11/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.5489 - auc: 0.9121 - acc: 0.8378 - positive: 2181.0000 - negative: 2184.0000 - precision: 0.8382 - recall: 0.8372\n",
      "Epoch 11: val_loss did not improve from 0.66965\n",
      "82/82 [==============================] - 193s 2s/step - loss: 0.5489 - auc: 0.9121 - acc: 0.8378 - positive: 2181.0000 - negative: 2184.0000 - precision: 0.8382 - recall: 0.8372 - val_loss: 0.7674 - val_auc: 0.8739 - val_acc: 0.7985 - val_positive: 491.0000 - val_negative: 480.0000 - val_precision: 0.7932 - val_recall: 0.8076 - lr: 1.0000e-04\n",
      "Epoch 12/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.5412 - auc: 0.9156 - acc: 0.8359 - positive: 2167.0000 - negative: 2188.0000 - precision: 0.8386 - recall: 0.8319\n",
      "Epoch 12: val_loss did not improve from 0.66965\n",
      "82/82 [==============================] - 194s 2s/step - loss: 0.5412 - auc: 0.9156 - acc: 0.8359 - positive: 2167.0000 - negative: 2188.0000 - precision: 0.8386 - recall: 0.8319 - val_loss: 0.7238 - val_auc: 0.8876 - val_acc: 0.8224 - val_positive: 516.0000 - val_negative: 484.0000 - val_precision: 0.8062 - val_recall: 0.8487 - lr: 1.0000e-04\n",
      "Epoch 13/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.5130 - auc: 0.9226 - acc: 0.8543 - positive: 2231.0000 - negative: 2220.0000 - precision: 0.8528 - recall: 0.8564\n",
      "Epoch 13: val_loss did not improve from 0.66965\n",
      "82/82 [==============================] - 190s 2s/step - loss: 0.5130 - auc: 0.9226 - acc: 0.8543 - positive: 2231.0000 - negative: 2220.0000 - precision: 0.8528 - recall: 0.8564 - val_loss: 0.7090 - val_auc: 0.8886 - val_acc: 0.8183 - val_positive: 489.0000 - val_negative: 506.0000 - val_precision: 0.8274 - val_recall: 0.8043 - lr: 1.0000e-04\n",
      "Epoch 14/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.4949 - auc: 0.9235 - acc: 0.8472 - positive: 2208.0000 - negative: 2206.0000 - precision: 0.8470 - recall: 0.8476\n",
      "Epoch 14: val_loss improved from 0.66965 to 0.61015, saving model to best_model.h5\n",
      "82/82 [==============================] - 190s 2s/step - loss: 0.4949 - auc: 0.9235 - acc: 0.8472 - positive: 2208.0000 - negative: 2206.0000 - precision: 0.8470 - recall: 0.8476 - val_loss: 0.6102 - val_auc: 0.9115 - val_acc: 0.8363 - val_positive: 531.0000 - val_negative: 486.0000 - val_precision: 0.8132 - val_recall: 0.8734 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.5034 - auc: 0.9230 - acc: 0.8509 - positive: 2207.0000 - negative: 2226.0000 - precision: 0.8534 - recall: 0.8472\n",
      "Epoch 15: val_loss did not improve from 0.61015\n",
      "82/82 [==============================] - 192s 2s/step - loss: 0.5034 - auc: 0.9230 - acc: 0.8509 - positive: 2207.0000 - negative: 2226.0000 - precision: 0.8534 - recall: 0.8472 - val_loss: 0.6862 - val_auc: 0.8911 - val_acc: 0.7993 - val_positive: 485.0000 - val_negative: 487.0000 - val_precision: 0.8003 - val_recall: 0.7977 - lr: 1.0000e-04\n",
      "Epoch 16/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.4769 - auc: 0.9288 - acc: 0.8509 - positive: 2228.0000 - negative: 2205.0000 - precision: 0.8478 - recall: 0.8553\n",
      "Epoch 16: val_loss did not improve from 0.61015\n",
      "82/82 [==============================] - 193s 2s/step - loss: 0.4769 - auc: 0.9288 - acc: 0.8509 - positive: 2228.0000 - negative: 2205.0000 - precision: 0.8478 - recall: 0.8553 - val_loss: 0.6380 - val_auc: 0.8978 - val_acc: 0.8141 - val_positive: 498.0000 - val_negative: 492.0000 - val_precision: 0.8111 - val_recall: 0.8191 - lr: 1.0000e-04\n",
      "Epoch 17/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.4946 - auc: 0.9253 - acc: 0.8489 - positive: 2207.0000 - negative: 2216.0000 - precision: 0.8502 - recall: 0.8472\n",
      "Epoch 17: val_loss did not improve from 0.61015\n",
      "82/82 [==============================] - 192s 2s/step - loss: 0.4946 - auc: 0.9253 - acc: 0.8489 - positive: 2207.0000 - negative: 2216.0000 - precision: 0.8502 - recall: 0.8472 - val_loss: 0.6103 - val_auc: 0.9120 - val_acc: 0.8363 - val_positive: 505.0000 - val_negative: 512.0000 - val_precision: 0.8403 - val_recall: 0.8306 - lr: 1.0000e-04\n",
      "Epoch 18/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.4743 - auc: 0.9302 - acc: 0.8559 - positive: 2228.0000 - negative: 2231.0000 - precision: 0.8563 - recall: 0.8553\n",
      "Epoch 18: val_loss improved from 0.61015 to 0.58087, saving model to best_model.h5\n",
      "82/82 [==============================] - 194s 2s/step - loss: 0.4743 - auc: 0.9302 - acc: 0.8559 - positive: 2228.0000 - negative: 2231.0000 - precision: 0.8563 - recall: 0.8553 - val_loss: 0.5809 - val_auc: 0.9075 - val_acc: 0.8174 - val_positive: 502.0000 - val_negative: 492.0000 - val_precision: 0.8123 - val_recall: 0.8257 - lr: 1.0000e-04\n",
      "Epoch 19/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.4770 - auc: 0.9278 - acc: 0.8539 - positive: 2229.0000 - negative: 2220.0000 - precision: 0.8527 - recall: 0.8557\n",
      "Epoch 19: val_loss did not improve from 0.58087\n",
      "82/82 [==============================] - 194s 2s/step - loss: 0.4770 - auc: 0.9278 - acc: 0.8539 - positive: 2229.0000 - negative: 2220.0000 - precision: 0.8527 - recall: 0.8557 - val_loss: 0.6648 - val_auc: 0.9035 - val_acc: 0.8257 - val_positive: 492.0000 - val_negative: 512.0000 - val_precision: 0.8367 - val_recall: 0.8092 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.4351 - auc: 0.9351 - acc: 0.8630 - positive: 2244.0000 - negative: 2252.0000 - precision: 0.8641 - recall: 0.8614\n",
      "Epoch 20: val_loss did not improve from 0.58087\n",
      "82/82 [==============================] - 194s 2s/step - loss: 0.4351 - auc: 0.9351 - acc: 0.8630 - positive: 2244.0000 - negative: 2252.0000 - precision: 0.8641 - recall: 0.8614 - val_loss: 0.6261 - val_auc: 0.9029 - val_acc: 0.8257 - val_positive: 495.0000 - val_negative: 509.0000 - val_precision: 0.8333 - val_recall: 0.8141 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.4447 - auc: 0.9334 - acc: 0.8580 - positive: 2236.0000 - negative: 2234.0000 - precision: 0.8577 - recall: 0.8583\n",
      "Epoch 21: val_loss did not improve from 0.58087\n",
      "82/82 [==============================] - 193s 2s/step - loss: 0.4447 - auc: 0.9334 - acc: 0.8580 - positive: 2236.0000 - negative: 2234.0000 - precision: 0.8577 - recall: 0.8583 - val_loss: 0.6016 - val_auc: 0.9088 - val_acc: 0.8281 - val_positive: 511.0000 - val_negative: 496.0000 - val_precision: 0.8202 - val_recall: 0.8405 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.4672 - auc: 0.9315 - acc: 0.8595 - positive: 2245.0000 - negative: 2233.0000 - precision: 0.8579 - recall: 0.8618\n",
      "Epoch 22: val_loss did not improve from 0.58087\n",
      "82/82 [==============================] - 194s 2s/step - loss: 0.4672 - auc: 0.9315 - acc: 0.8595 - positive: 2245.0000 - negative: 2233.0000 - precision: 0.8579 - recall: 0.8618 - val_loss: 0.6516 - val_auc: 0.8977 - val_acc: 0.8199 - val_positive: 497.0000 - val_negative: 500.0000 - val_precision: 0.8215 - val_recall: 0.8174 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.4241 - auc: 0.9376 - acc: 0.8693 - positive: 2266.0000 - negative: 2263.0000 - precision: 0.8689 - recall: 0.8699\n",
      "Epoch 23: val_loss improved from 0.58087 to 0.54494, saving model to best_model.h5\n",
      "82/82 [==============================] - 199s 2s/step - loss: 0.4241 - auc: 0.9376 - acc: 0.8693 - positive: 2266.0000 - negative: 2263.0000 - precision: 0.8689 - recall: 0.8699 - val_loss: 0.5449 - val_auc: 0.9154 - val_acc: 0.8331 - val_positive: 514.0000 - val_negative: 499.0000 - val_precision: 0.8250 - val_recall: 0.8454 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.4205 - auc: 0.9361 - acc: 0.8631 - positive: 2242.0000 - negative: 2255.0000 - precision: 0.8650 - recall: 0.8607\n",
      "Epoch 24: val_loss improved from 0.54494 to 0.54093, saving model to best_model.h5\n",
      "82/82 [==============================] - 193s 2s/step - loss: 0.4205 - auc: 0.9361 - acc: 0.8631 - positive: 2242.0000 - negative: 2255.0000 - precision: 0.8650 - recall: 0.8607 - val_loss: 0.5409 - val_auc: 0.9107 - val_acc: 0.8405 - val_positive: 503.0000 - val_negative: 519.0000 - val_precision: 0.8497 - val_recall: 0.8273 - lr: 1.0000e-04\n",
      "Epoch 25/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.4131 - auc: 0.9404 - acc: 0.8668 - positive: 2261.0000 - negative: 2255.0000 - precision: 0.8660 - recall: 0.8679\n",
      "Epoch 25: val_loss did not improve from 0.54093\n",
      "82/82 [==============================] - 193s 2s/step - loss: 0.4131 - auc: 0.9404 - acc: 0.8668 - positive: 2261.0000 - negative: 2255.0000 - precision: 0.8660 - recall: 0.8679 - val_loss: 0.5526 - val_auc: 0.9143 - val_acc: 0.8314 - val_positive: 489.0000 - val_negative: 522.0000 - val_precision: 0.8504 - val_recall: 0.8043 - lr: 1.0000e-04\n",
      "Epoch 26/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.4388 - auc: 0.9346 - acc: 0.8595 - positive: 2235.0000 - negative: 2243.0000 - precision: 0.8606 - recall: 0.8580\n",
      "Epoch 26: val_loss did not improve from 0.54093\n",
      "82/82 [==============================] - 194s 2s/step - loss: 0.4388 - auc: 0.9346 - acc: 0.8595 - positive: 2235.0000 - negative: 2243.0000 - precision: 0.8606 - recall: 0.8580 - val_loss: 0.5989 - val_auc: 0.9094 - val_acc: 0.8265 - val_positive: 487.0000 - val_negative: 518.0000 - val_precision: 0.8440 - val_recall: 0.8010 - lr: 1.0000e-04\n",
      "Epoch 27/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.4161 - auc: 0.9397 - acc: 0.8618 - positive: 2241.0000 - negative: 2249.0000 - precision: 0.8629 - recall: 0.8603\n",
      "Epoch 27: val_loss did not improve from 0.54093\n",
      "82/82 [==============================] - 191s 2s/step - loss: 0.4161 - auc: 0.9397 - acc: 0.8618 - positive: 2241.0000 - negative: 2249.0000 - precision: 0.8629 - recall: 0.8603 - val_loss: 0.6044 - val_auc: 0.9033 - val_acc: 0.8322 - val_positive: 503.0000 - val_negative: 509.0000 - val_precision: 0.8355 - val_recall: 0.8273 - lr: 1.0000e-04\n",
      "Epoch 28/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.4216 - auc: 0.9380 - acc: 0.8645 - positive: 2254.0000 - negative: 2250.0000 - precision: 0.8639 - recall: 0.8653\n",
      "Epoch 28: val_loss did not improve from 0.54093\n",
      "82/82 [==============================] - 193s 2s/step - loss: 0.4216 - auc: 0.9380 - acc: 0.8645 - positive: 2254.0000 - negative: 2250.0000 - precision: 0.8639 - recall: 0.8653 - val_loss: 0.5954 - val_auc: 0.9112 - val_acc: 0.8265 - val_positive: 511.0000 - val_negative: 494.0000 - val_precision: 0.8176 - val_recall: 0.8405 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.4019 - auc: 0.9417 - acc: 0.8724 - positive: 2273.0000 - negative: 2272.0000 - precision: 0.8722 - recall: 0.8726\n",
      "Epoch 29: val_loss did not improve from 0.54093\n",
      "82/82 [==============================] - 192s 2s/step - loss: 0.4019 - auc: 0.9417 - acc: 0.8724 - positive: 2273.0000 - negative: 2272.0000 - precision: 0.8722 - recall: 0.8726 - val_loss: 0.5821 - val_auc: 0.9091 - val_acc: 0.8240 - val_positive: 504.0000 - val_negative: 498.0000 - val_precision: 0.8208 - val_recall: 0.8289 - lr: 1.0000e-04\n",
      "Epoch 30/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.4113 - auc: 0.9393 - acc: 0.8643 - positive: 2245.0000 - negative: 2258.0000 - precision: 0.8661 - recall: 0.8618\n",
      "Epoch 30: val_loss did not improve from 0.54093\n",
      "82/82 [==============================] - 194s 2s/step - loss: 0.4113 - auc: 0.9393 - acc: 0.8643 - positive: 2245.0000 - negative: 2258.0000 - precision: 0.8661 - recall: 0.8618 - val_loss: 0.6354 - val_auc: 0.8995 - val_acc: 0.8248 - val_positive: 506.0000 - val_negative: 497.0000 - val_precision: 0.8201 - val_recall: 0.8322 - lr: 1.0000e-04\n",
      "Epoch 31/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3906 - auc: 0.9425 - acc: 0.8747 - positive: 2270.0000 - negative: 2287.0000 - precision: 0.8771 - recall: 0.8714\n",
      "Epoch 31: val_loss did not improve from 0.54093\n",
      "82/82 [==============================] - 193s 2s/step - loss: 0.3906 - auc: 0.9425 - acc: 0.8747 - positive: 2270.0000 - negative: 2287.0000 - precision: 0.8771 - recall: 0.8714 - val_loss: 0.5939 - val_auc: 0.9103 - val_acc: 0.8322 - val_positive: 511.0000 - val_negative: 501.0000 - val_precision: 0.8269 - val_recall: 0.8405 - lr: 1.0000e-04\n",
      "Epoch 32/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.4001 - auc: 0.9410 - acc: 0.8685 - positive: 2267.0000 - negative: 2258.0000 - precision: 0.8673 - recall: 0.8702\n",
      "Epoch 32: val_loss did not improve from 0.54093\n",
      "82/82 [==============================] - 194s 2s/step - loss: 0.4001 - auc: 0.9410 - acc: 0.8685 - positive: 2267.0000 - negative: 2258.0000 - precision: 0.8673 - recall: 0.8702 - val_loss: 0.6610 - val_auc: 0.9056 - val_acc: 0.8339 - val_positive: 519.0000 - val_negative: 495.0000 - val_precision: 0.8212 - val_recall: 0.8536 - lr: 1.0000e-04\n",
      "Epoch 33/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3900 - auc: 0.9432 - acc: 0.8716 - positive: 2262.0000 - negative: 2279.0000 - precision: 0.8740 - recall: 0.8683\n",
      "Epoch 33: val_loss improved from 0.54093 to 0.50132, saving model to best_model.h5\n",
      "82/82 [==============================] - 192s 2s/step - loss: 0.3900 - auc: 0.9432 - acc: 0.8716 - positive: 2262.0000 - negative: 2279.0000 - precision: 0.8740 - recall: 0.8683 - val_loss: 0.5013 - val_auc: 0.9230 - val_acc: 0.8429 - val_positive: 515.0000 - val_negative: 510.0000 - val_precision: 0.8401 - val_recall: 0.8470 - lr: 1.0000e-04\n",
      "Epoch 34/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.4072 - auc: 0.9412 - acc: 0.8704 - positive: 2277.0000 - negative: 2258.0000 - precision: 0.8678 - recall: 0.8741\n",
      "Epoch 34: val_loss did not improve from 0.50132\n",
      "82/82 [==============================] - 193s 2s/step - loss: 0.4072 - auc: 0.9412 - acc: 0.8704 - positive: 2277.0000 - negative: 2258.0000 - precision: 0.8678 - recall: 0.8741 - val_loss: 0.5810 - val_auc: 0.9108 - val_acc: 0.8363 - val_positive: 495.0000 - val_negative: 522.0000 - val_precision: 0.8520 - val_recall: 0.8141 - lr: 1.0000e-04\n",
      "Epoch 35/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3580 - auc: 0.9497 - acc: 0.8816 - positive: 2288.0000 - negative: 2305.0000 - precision: 0.8841 - recall: 0.8783\n",
      "Epoch 35: val_loss did not improve from 0.50132\n",
      "82/82 [==============================] - 194s 2s/step - loss: 0.3580 - auc: 0.9497 - acc: 0.8816 - positive: 2288.0000 - negative: 2305.0000 - precision: 0.8841 - recall: 0.8783 - val_loss: 0.5630 - val_auc: 0.9182 - val_acc: 0.8355 - val_positive: 517.0000 - val_negative: 499.0000 - val_precision: 0.8259 - val_recall: 0.8503 - lr: 1.0000e-04\n",
      "Epoch 36/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3927 - auc: 0.9432 - acc: 0.8724 - positive: 2259.0000 - negative: 2286.0000 - precision: 0.8763 - recall: 0.8672\n",
      "Epoch 36: val_loss did not improve from 0.50132\n",
      "82/82 [==============================] - 192s 2s/step - loss: 0.3927 - auc: 0.9432 - acc: 0.8724 - positive: 2259.0000 - negative: 2286.0000 - precision: 0.8763 - recall: 0.8672 - val_loss: 0.5465 - val_auc: 0.9212 - val_acc: 0.8520 - val_positive: 509.0000 - val_negative: 527.0000 - val_precision: 0.8627 - val_recall: 0.8372 - lr: 1.0000e-04\n",
      "Epoch 37/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3648 - auc: 0.9490 - acc: 0.8812 - positive: 2287.0000 - negative: 2304.0000 - precision: 0.8837 - recall: 0.8779\n",
      "Epoch 37: val_loss did not improve from 0.50132\n",
      "82/82 [==============================] - 193s 2s/step - loss: 0.3648 - auc: 0.9490 - acc: 0.8812 - positive: 2287.0000 - negative: 2304.0000 - precision: 0.8837 - recall: 0.8779 - val_loss: 0.5763 - val_auc: 0.9113 - val_acc: 0.8306 - val_positive: 494.0000 - val_negative: 516.0000 - val_precision: 0.8430 - val_recall: 0.8125 - lr: 1.0000e-04\n",
      "Epoch 38/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3840 - auc: 0.9445 - acc: 0.8762 - positive: 2279.0000 - negative: 2286.0000 - precision: 0.8772 - recall: 0.8749\n",
      "Epoch 38: val_loss did not improve from 0.50132\n",
      "82/82 [==============================] - 192s 2s/step - loss: 0.3840 - auc: 0.9445 - acc: 0.8762 - positive: 2279.0000 - negative: 2286.0000 - precision: 0.8772 - recall: 0.8749 - val_loss: 0.5997 - val_auc: 0.9190 - val_acc: 0.8479 - val_positive: 518.0000 - val_negative: 513.0000 - val_precision: 0.8450 - val_recall: 0.8520 - lr: 1.0000e-04\n",
      "Epoch 39/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3857 - auc: 0.9440 - acc: 0.8747 - positive: 2280.0000 - negative: 2277.0000 - precision: 0.8742 - recall: 0.8752\n",
      "Epoch 39: val_loss did not improve from 0.50132\n",
      "82/82 [==============================] - 192s 2s/step - loss: 0.3857 - auc: 0.9440 - acc: 0.8747 - positive: 2280.0000 - negative: 2277.0000 - precision: 0.8742 - recall: 0.8752 - val_loss: 0.5201 - val_auc: 0.9211 - val_acc: 0.8470 - val_positive: 499.0000 - val_negative: 531.0000 - val_precision: 0.8663 - val_recall: 0.8207 - lr: 1.0000e-04\n",
      "Epoch 40/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3717 - auc: 0.9460 - acc: 0.8745 - positive: 2270.0000 - negative: 2286.0000 - precision: 0.8768 - recall: 0.8714\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.50132\n",
      "82/82 [==============================] - 193s 2s/step - loss: 0.3717 - auc: 0.9460 - acc: 0.8745 - positive: 2270.0000 - negative: 2286.0000 - precision: 0.8768 - recall: 0.8714 - val_loss: 0.6011 - val_auc: 0.9016 - val_acc: 0.8322 - val_positive: 518.0000 - val_negative: 494.0000 - val_precision: 0.8196 - val_recall: 0.8520 - lr: 1.0000e-04\n",
      "Epoch 41/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3394 - auc: 0.9513 - acc: 0.8841 - positive: 2307.0000 - negative: 2299.0000 - precision: 0.8829 - recall: 0.8856\n",
      "Epoch 41: val_loss did not improve from 0.50132\n",
      "82/82 [==============================] - 193s 2s/step - loss: 0.3394 - auc: 0.9513 - acc: 0.8841 - positive: 2307.0000 - negative: 2299.0000 - precision: 0.8829 - recall: 0.8856 - val_loss: 0.5570 - val_auc: 0.9127 - val_acc: 0.8413 - val_positive: 509.0000 - val_negative: 514.0000 - val_precision: 0.8441 - val_recall: 0.8372 - lr: 2.0000e-05\n",
      "Epoch 42/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3468 - auc: 0.9505 - acc: 0.8766 - positive: 2281.0000 - negative: 2286.0000 - precision: 0.8773 - recall: 0.8756\n",
      "Epoch 42: val_loss did not improve from 0.50132\n",
      "82/82 [==============================] - 194s 2s/step - loss: 0.3468 - auc: 0.9505 - acc: 0.8766 - positive: 2281.0000 - negative: 2286.0000 - precision: 0.8773 - recall: 0.8756 - val_loss: 0.5735 - val_auc: 0.9148 - val_acc: 0.8339 - val_positive: 509.0000 - val_negative: 505.0000 - val_precision: 0.8317 - val_recall: 0.8372 - lr: 2.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3348 - auc: 0.9524 - acc: 0.8871 - positive: 2311.0000 - negative: 2311.0000 - precision: 0.8871 - recall: 0.8871\n",
      "Epoch 43: val_loss did not improve from 0.50132\n",
      "82/82 [==============================] - 192s 2s/step - loss: 0.3348 - auc: 0.9524 - acc: 0.8871 - positive: 2311.0000 - negative: 2311.0000 - precision: 0.8871 - recall: 0.8871 - val_loss: 0.5050 - val_auc: 0.9234 - val_acc: 0.8413 - val_positive: 515.0000 - val_negative: 508.0000 - val_precision: 0.8374 - val_recall: 0.8470 - lr: 2.0000e-05\n",
      "Epoch 44/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3266 - auc: 0.9544 - acc: 0.8891 - positive: 2326.0000 - negative: 2306.0000 - precision: 0.8861 - recall: 0.8929\n",
      "Epoch 44: val_loss did not improve from 0.50132\n",
      "82/82 [==============================] - 194s 2s/step - loss: 0.3266 - auc: 0.9544 - acc: 0.8891 - positive: 2326.0000 - negative: 2306.0000 - precision: 0.8861 - recall: 0.8929 - val_loss: 0.5255 - val_auc: 0.9211 - val_acc: 0.8438 - val_positive: 504.0000 - val_negative: 522.0000 - val_precision: 0.8542 - val_recall: 0.8289 - lr: 2.0000e-05\n",
      "Epoch 45/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3313 - auc: 0.9532 - acc: 0.8789 - positive: 2295.0000 - negative: 2284.0000 - precision: 0.8773 - recall: 0.8810\n",
      "Epoch 45: val_loss did not improve from 0.50132\n",
      "82/82 [==============================] - 191s 2s/step - loss: 0.3313 - auc: 0.9532 - acc: 0.8789 - positive: 2295.0000 - negative: 2284.0000 - precision: 0.8773 - recall: 0.8810 - val_loss: 0.5042 - val_auc: 0.9261 - val_acc: 0.8627 - val_positive: 528.0000 - val_negative: 521.0000 - val_precision: 0.8585 - val_recall: 0.8684 - lr: 2.0000e-05\n",
      "Epoch 46/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3307 - auc: 0.9545 - acc: 0.8803 - positive: 2309.0000 - negative: 2311.0000 - precision: 0.8806 - recall: 0.8800\n",
      "Epoch 46: val_loss improved from 0.50132 to 0.46764, saving model to best_model.h5\n",
      "82/82 [==============================] - 198s 2s/step - loss: 0.3307 - auc: 0.9545 - acc: 0.8803 - positive: 2309.0000 - negative: 2311.0000 - precision: 0.8806 - recall: 0.8800 - val_loss: 0.4676 - val_auc: 0.9322 - val_acc: 0.8577 - val_positive: 521.0000 - val_negative: 522.0000 - val_precision: 0.8583 - val_recall: 0.8569 - lr: 2.0000e-05\n",
      "Epoch 47/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3301 - auc: 0.9540 - acc: 0.8835 - positive: 2296.0000 - negative: 2307.0000 - precision: 0.8851 - recall: 0.8814\n",
      "Epoch 47: val_loss did not improve from 0.46764\n",
      "82/82 [==============================] - 194s 2s/step - loss: 0.3301 - auc: 0.9540 - acc: 0.8835 - positive: 2296.0000 - negative: 2307.0000 - precision: 0.8851 - recall: 0.8814 - val_loss: 0.5228 - val_auc: 0.9227 - val_acc: 0.8487 - val_positive: 523.0000 - val_negative: 509.0000 - val_precision: 0.8408 - val_recall: 0.8602 - lr: 2.0000e-05\n",
      "Epoch 48/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3364 - auc: 0.9538 - acc: 0.8848 - positive: 2295.0000 - negative: 2315.0000 - precision: 0.8878 - recall: 0.8810\n",
      "Epoch 48: val_loss did not improve from 0.46764\n",
      "82/82 [==============================] - 197s 2s/step - loss: 0.3364 - auc: 0.9538 - acc: 0.8848 - positive: 2295.0000 - negative: 2315.0000 - precision: 0.8878 - recall: 0.8810 - val_loss: 0.4801 - val_auc: 0.9259 - val_acc: 0.8446 - val_positive: 510.0000 - val_negative: 517.0000 - val_precision: 0.8486 - val_recall: 0.8388 - lr: 2.0000e-05\n",
      "Epoch 49/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3088 - auc: 0.9572 - acc: 0.8919 - positive: 2319.0000 - negative: 2328.0000 - precision: 0.8933 - recall: 0.8902\n",
      "Epoch 49: val_loss did not improve from 0.46764\n",
      "82/82 [==============================] - 193s 2s/step - loss: 0.3088 - auc: 0.9572 - acc: 0.8919 - positive: 2319.0000 - negative: 2328.0000 - precision: 0.8933 - recall: 0.8902 - val_loss: 0.4837 - val_auc: 0.9294 - val_acc: 0.8520 - val_positive: 509.0000 - val_negative: 527.0000 - val_precision: 0.8627 - val_recall: 0.8372 - lr: 2.0000e-05\n",
      "Epoch 50/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3073 - auc: 0.9578 - acc: 0.8873 - positive: 2316.0000 - negative: 2307.0000 - precision: 0.8860 - recall: 0.8891\n",
      "Epoch 50: val_loss did not improve from 0.46764\n",
      "82/82 [==============================] - 195s 2s/step - loss: 0.3073 - auc: 0.9578 - acc: 0.8873 - positive: 2316.0000 - negative: 2307.0000 - precision: 0.8860 - recall: 0.8891 - val_loss: 0.5290 - val_auc: 0.9190 - val_acc: 0.8462 - val_positive: 511.0000 - val_negative: 518.0000 - val_precision: 0.8502 - val_recall: 0.8405 - lr: 2.0000e-05\n",
      "Epoch 51/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3163 - auc: 0.9564 - acc: 0.8860 - positive: 2314.0000 - negative: 2302.0000 - precision: 0.8842 - recall: 0.8883\n",
      "Epoch 51: val_loss did not improve from 0.46764\n",
      "82/82 [==============================] - 193s 2s/step - loss: 0.3163 - auc: 0.9564 - acc: 0.8860 - positive: 2314.0000 - negative: 2302.0000 - precision: 0.8842 - recall: 0.8883 - val_loss: 0.5210 - val_auc: 0.9173 - val_acc: 0.8396 - val_positive: 506.0000 - val_negative: 515.0000 - val_precision: 0.8447 - val_recall: 0.8322 - lr: 2.0000e-05\n",
      "Epoch 52/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3180 - auc: 0.9554 - acc: 0.8770 - positive: 2281.0000 - negative: 2288.0000 - precision: 0.8780 - recall: 0.8756\n",
      "Epoch 52: val_loss did not improve from 0.46764\n",
      "82/82 [==============================] - 195s 2s/step - loss: 0.3180 - auc: 0.9554 - acc: 0.8770 - positive: 2281.0000 - negative: 2288.0000 - precision: 0.8780 - recall: 0.8756 - val_loss: 0.4994 - val_auc: 0.9253 - val_acc: 0.8528 - val_positive: 523.0000 - val_negative: 514.0000 - val_precision: 0.8476 - val_recall: 0.8602 - lr: 2.0000e-05\n",
      "Epoch 53/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.2999 - auc: 0.9575 - acc: 0.8893 - positive: 2319.0000 - negative: 2314.0000 - precision: 0.8885 - recall: 0.8902\n",
      "Epoch 53: val_loss did not improve from 0.46764\n",
      "82/82 [==============================] - 192s 2s/step - loss: 0.2999 - auc: 0.9575 - acc: 0.8893 - positive: 2319.0000 - negative: 2314.0000 - precision: 0.8885 - recall: 0.8902 - val_loss: 0.5510 - val_auc: 0.9123 - val_acc: 0.8298 - val_positive: 503.0000 - val_negative: 506.0000 - val_precision: 0.8314 - val_recall: 0.8273 - lr: 2.0000e-05\n",
      "Epoch 54/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.2974 - auc: 0.9595 - acc: 0.8885 - positive: 2315.0000 - negative: 2314.0000 - precision: 0.8883 - recall: 0.8887\n",
      "Epoch 54: val_loss did not improve from 0.46764\n",
      "82/82 [==============================] - 195s 2s/step - loss: 0.2974 - auc: 0.9595 - acc: 0.8885 - positive: 2315.0000 - negative: 2314.0000 - precision: 0.8883 - recall: 0.8887 - val_loss: 0.5527 - val_auc: 0.9134 - val_acc: 0.8363 - val_positive: 509.0000 - val_negative: 508.0000 - val_precision: 0.8358 - val_recall: 0.8372 - lr: 2.0000e-05\n",
      "Epoch 55/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3309 - auc: 0.9527 - acc: 0.8795 - positive: 2296.0000 - negative: 2286.0000 - precision: 0.8780 - recall: 0.8814\n",
      "Epoch 55: val_loss improved from 0.46764 to 0.44071, saving model to best_model.h5\n",
      "82/82 [==============================] - 193s 2s/step - loss: 0.3309 - auc: 0.9527 - acc: 0.8795 - positive: 2296.0000 - negative: 2286.0000 - precision: 0.8780 - recall: 0.8814 - val_loss: 0.4407 - val_auc: 0.9324 - val_acc: 0.8462 - val_positive: 501.0000 - val_negative: 528.0000 - val_precision: 0.8623 - val_recall: 0.8240 - lr: 2.0000e-05\n",
      "Epoch 56/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.2959 - auc: 0.9592 - acc: 0.8902 - positive: 2332.0000 - negative: 2340.0000 - precision: 0.8914 - recall: 0.8887\n",
      "Epoch 56: val_loss did not improve from 0.44071\n",
      "82/82 [==============================] - 195s 2s/step - loss: 0.2959 - auc: 0.9592 - acc: 0.8902 - positive: 2332.0000 - negative: 2340.0000 - precision: 0.8914 - recall: 0.8887 - val_loss: 0.4728 - val_auc: 0.9287 - val_acc: 0.8602 - val_positive: 533.0000 - val_negative: 513.0000 - val_precision: 0.8487 - val_recall: 0.8766 - lr: 2.0000e-05\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82/82 [==============================] - ETA: 0s - loss: 0.2985 - auc: 0.9588 - acc: 0.8948 - positive: 2332.0000 - negative: 2330.0000 - precision: 0.8945 - recall: 0.8952\n",
      "Epoch 57: val_loss did not improve from 0.44071\n",
      "82/82 [==============================] - 190s 2s/step - loss: 0.2985 - auc: 0.9588 - acc: 0.8948 - positive: 2332.0000 - negative: 2330.0000 - precision: 0.8945 - recall: 0.8952 - val_loss: 0.4686 - val_auc: 0.9307 - val_acc: 0.8544 - val_positive: 524.0000 - val_negative: 515.0000 - val_precision: 0.8493 - val_recall: 0.8618 - lr: 2.0000e-05\n",
      "Epoch 58/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3082 - auc: 0.9568 - acc: 0.8825 - positive: 2297.0000 - negative: 2301.0000 - precision: 0.8831 - recall: 0.8818\n",
      "Epoch 58: val_loss did not improve from 0.44071\n",
      "82/82 [==============================] - 194s 2s/step - loss: 0.3082 - auc: 0.9568 - acc: 0.8825 - positive: 2297.0000 - negative: 2301.0000 - precision: 0.8831 - recall: 0.8818 - val_loss: 0.5285 - val_auc: 0.9149 - val_acc: 0.8405 - val_positive: 509.0000 - val_negative: 513.0000 - val_precision: 0.8427 - val_recall: 0.8372 - lr: 2.0000e-05\n",
      "Epoch 59/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3058 - auc: 0.9571 - acc: 0.8860 - positive: 2313.0000 - negative: 2303.0000 - precision: 0.8845 - recall: 0.8879\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 3.999999898951501e-06.\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.44071\n",
      "82/82 [==============================] - 192s 2s/step - loss: 0.3058 - auc: 0.9571 - acc: 0.8860 - positive: 2313.0000 - negative: 2303.0000 - precision: 0.8845 - recall: 0.8879 - val_loss: 0.4891 - val_auc: 0.9243 - val_acc: 0.8306 - val_positive: 509.0000 - val_negative: 501.0000 - val_precision: 0.8263 - val_recall: 0.8372 - lr: 2.0000e-05\n",
      "Epoch 60/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3118 - auc: 0.9567 - acc: 0.8866 - positive: 2319.0000 - negative: 2300.0000 - precision: 0.8838 - recall: 0.8902\n",
      "Epoch 60: val_loss did not improve from 0.44071\n",
      "82/82 [==============================] - 194s 2s/step - loss: 0.3118 - auc: 0.9567 - acc: 0.8866 - positive: 2319.0000 - negative: 2300.0000 - precision: 0.8838 - recall: 0.8902 - val_loss: 0.5428 - val_auc: 0.9162 - val_acc: 0.8396 - val_positive: 504.0000 - val_negative: 517.0000 - val_precision: 0.8471 - val_recall: 0.8289 - lr: 4.0000e-06\n",
      "Epoch 61/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.2857 - auc: 0.9610 - acc: 0.8917 - positive: 2323.0000 - negative: 2323.0000 - precision: 0.8917 - recall: 0.8917\n",
      "Epoch 61: val_loss did not improve from 0.44071\n",
      "82/82 [==============================] - 194s 2s/step - loss: 0.2857 - auc: 0.9610 - acc: 0.8917 - positive: 2323.0000 - negative: 2323.0000 - precision: 0.8917 - recall: 0.8917 - val_loss: 0.5347 - val_auc: 0.9187 - val_acc: 0.8470 - val_positive: 521.0000 - val_negative: 509.0000 - val_precision: 0.8403 - val_recall: 0.8569 - lr: 4.0000e-06\n",
      "Epoch 62/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3247 - auc: 0.9546 - acc: 0.8845 - positive: 2307.0000 - negative: 2301.0000 - precision: 0.8836 - recall: 0.8856\n",
      "Epoch 62: val_loss did not improve from 0.44071\n",
      "82/82 [==============================] - 195s 2s/step - loss: 0.3247 - auc: 0.9546 - acc: 0.8845 - positive: 2307.0000 - negative: 2301.0000 - precision: 0.8836 - recall: 0.8856 - val_loss: 0.5375 - val_auc: 0.9128 - val_acc: 0.8380 - val_positive: 505.0000 - val_negative: 514.0000 - val_precision: 0.8431 - val_recall: 0.8306 - lr: 4.0000e-06\n",
      "Epoch 63/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3253 - auc: 0.9536 - acc: 0.8885 - positive: 2307.0000 - negative: 2322.0000 - precision: 0.8907 - recall: 0.8856\n",
      "Epoch 63: val_loss did not improve from 0.44071\n",
      "82/82 [==============================] - 191s 2s/step - loss: 0.3253 - auc: 0.9536 - acc: 0.8885 - positive: 2307.0000 - negative: 2322.0000 - precision: 0.8907 - recall: 0.8856 - val_loss: 0.4553 - val_auc: 0.9286 - val_acc: 0.8470 - val_positive: 516.0000 - val_negative: 514.0000 - val_precision: 0.8459 - val_recall: 0.8487 - lr: 4.0000e-06\n",
      "Epoch 64/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3010 - auc: 0.9580 - acc: 0.8893 - positive: 2313.0000 - negative: 2320.0000 - precision: 0.8903 - recall: 0.8879\n",
      "Epoch 64: val_loss did not improve from 0.44071\n",
      "82/82 [==============================] - 196s 2s/step - loss: 0.3010 - auc: 0.9580 - acc: 0.8893 - positive: 2313.0000 - negative: 2320.0000 - precision: 0.8903 - recall: 0.8879 - val_loss: 0.5676 - val_auc: 0.9111 - val_acc: 0.8339 - val_positive: 510.0000 - val_negative: 504.0000 - val_precision: 0.8306 - val_recall: 0.8388 - lr: 4.0000e-06\n",
      "Epoch 65/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.2884 - auc: 0.9609 - acc: 0.8908 - positive: 2327.0000 - negative: 2314.0000 - precision: 0.8888 - recall: 0.8933\n",
      "Epoch 65: val_loss did not improve from 0.44071\n",
      "82/82 [==============================] - 192s 2s/step - loss: 0.2884 - auc: 0.9609 - acc: 0.8908 - positive: 2327.0000 - negative: 2314.0000 - precision: 0.8888 - recall: 0.8933 - val_loss: 0.4824 - val_auc: 0.9213 - val_acc: 0.8495 - val_positive: 518.0000 - val_negative: 515.0000 - val_precision: 0.8478 - val_recall: 0.8520 - lr: 4.0000e-06\n",
      "Epoch 66/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.2939 - auc: 0.9595 - acc: 0.8873 - positive: 2306.0000 - negative: 2317.0000 - precision: 0.8890 - recall: 0.8852\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 7.999999979801942e-07.\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.44071\n",
      "82/82 [==============================] - 193s 2s/step - loss: 0.2939 - auc: 0.9595 - acc: 0.8873 - positive: 2306.0000 - negative: 2317.0000 - precision: 0.8890 - recall: 0.8852 - val_loss: 0.5822 - val_auc: 0.9081 - val_acc: 0.8273 - val_positive: 498.0000 - val_negative: 508.0000 - val_precision: 0.8328 - val_recall: 0.8191 - lr: 4.0000e-06\n",
      "Epoch 67/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.2862 - auc: 0.9608 - acc: 0.8962 - positive: 2330.0000 - negative: 2339.0000 - precision: 0.8975 - recall: 0.8944\n",
      "Epoch 67: val_loss did not improve from 0.44071\n",
      "82/82 [==============================] - 194s 2s/step - loss: 0.2862 - auc: 0.9608 - acc: 0.8962 - positive: 2330.0000 - negative: 2339.0000 - precision: 0.8975 - recall: 0.8944 - val_loss: 0.4517 - val_auc: 0.9321 - val_acc: 0.8512 - val_positive: 520.0000 - val_negative: 515.0000 - val_precision: 0.8483 - val_recall: 0.8553 - lr: 8.0000e-07\n",
      "Epoch 68/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.2886 - auc: 0.9601 - acc: 0.8912 - positive: 2326.0000 - negative: 2317.0000 - precision: 0.8898 - recall: 0.8929\n",
      "Epoch 68: val_loss did not improve from 0.44071\n",
      "82/82 [==============================] - 194s 2s/step - loss: 0.2886 - auc: 0.9601 - acc: 0.8912 - positive: 2326.0000 - negative: 2317.0000 - precision: 0.8898 - recall: 0.8929 - val_loss: 0.5055 - val_auc: 0.9294 - val_acc: 0.8454 - val_positive: 516.0000 - val_negative: 512.0000 - val_precision: 0.8431 - val_recall: 0.8487 - lr: 8.0000e-07\n",
      "Epoch 69/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.2951 - auc: 0.9598 - acc: 0.8902 - positive: 2335.0000 - negative: 2303.0000 - precision: 0.8855 - recall: 0.8964\n",
      "Epoch 69: val_loss did not improve from 0.44071\n",
      "82/82 [==============================] - 193s 2s/step - loss: 0.2951 - auc: 0.9598 - acc: 0.8902 - positive: 2335.0000 - negative: 2303.0000 - precision: 0.8855 - recall: 0.8964 - val_loss: 0.4765 - val_auc: 0.9277 - val_acc: 0.8544 - val_positive: 517.0000 - val_negative: 522.0000 - val_precision: 0.8574 - val_recall: 0.8503 - lr: 8.0000e-07\n",
      "Epoch 70/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3103 - auc: 0.9570 - acc: 0.8896 - positive: 2292.0000 - negative: 2343.0000 - precision: 0.8974 - recall: 0.8798\n",
      "Epoch 70: val_loss did not improve from 0.44071\n",
      "82/82 [==============================] - 197s 2s/step - loss: 0.3103 - auc: 0.9570 - acc: 0.8896 - positive: 2292.0000 - negative: 2343.0000 - precision: 0.8974 - recall: 0.8798 - val_loss: 0.4991 - val_auc: 0.9180 - val_acc: 0.8339 - val_positive: 504.0000 - val_negative: 510.0000 - val_precision: 0.8372 - val_recall: 0.8289 - lr: 8.0000e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.2970 - auc: 0.9590 - acc: 0.8914 - positive: 2328.0000 - negative: 2316.0000 - precision: 0.8896 - recall: 0.8937\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 1.600000018697756e-07.\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.44071\n",
      "82/82 [==============================] - 193s 2s/step - loss: 0.2970 - auc: 0.9590 - acc: 0.8914 - positive: 2328.0000 - negative: 2316.0000 - precision: 0.8896 - recall: 0.8937 - val_loss: 0.5042 - val_auc: 0.9192 - val_acc: 0.8396 - val_positive: 510.0000 - val_negative: 511.0000 - val_precision: 0.8402 - val_recall: 0.8388 - lr: 8.0000e-07\n",
      "Epoch 72/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.2781 - auc: 0.9616 - acc: 0.8906 - positive: 2312.0000 - negative: 2328.0000 - precision: 0.8930 - recall: 0.8875\n",
      "Epoch 72: val_loss did not improve from 0.44071\n",
      "82/82 [==============================] - 197s 2s/step - loss: 0.2781 - auc: 0.9616 - acc: 0.8906 - positive: 2312.0000 - negative: 2328.0000 - precision: 0.8930 - recall: 0.8875 - val_loss: 0.4970 - val_auc: 0.9240 - val_acc: 0.8421 - val_positive: 513.0000 - val_negative: 511.0000 - val_precision: 0.8410 - val_recall: 0.8438 - lr: 1.6000e-07\n",
      "Epoch 73/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.2817 - auc: 0.9626 - acc: 0.8937 - positive: 2343.0000 - negative: 2313.0000 - precision: 0.8892 - recall: 0.8994\n",
      "Epoch 73: val_loss did not improve from 0.44071\n",
      "82/82 [==============================] - 193s 2s/step - loss: 0.2817 - auc: 0.9626 - acc: 0.8937 - positive: 2343.0000 - negative: 2313.0000 - precision: 0.8892 - recall: 0.8994 - val_loss: 0.5141 - val_auc: 0.9190 - val_acc: 0.8429 - val_positive: 515.0000 - val_negative: 510.0000 - val_precision: 0.8401 - val_recall: 0.8470 - lr: 1.6000e-07\n",
      "Epoch 74/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3060 - auc: 0.9576 - acc: 0.8881 - positive: 2319.0000 - negative: 2308.0000 - precision: 0.8865 - recall: 0.8902\n",
      "Epoch 74: val_loss improved from 0.44071 to 0.42846, saving model to best_model.h5\n",
      "82/82 [==============================] - 194s 2s/step - loss: 0.3060 - auc: 0.9576 - acc: 0.8881 - positive: 2319.0000 - negative: 2308.0000 - precision: 0.8865 - recall: 0.8902 - val_loss: 0.4285 - val_auc: 0.9353 - val_acc: 0.8544 - val_positive: 515.0000 - val_negative: 524.0000 - val_precision: 0.8598 - val_recall: 0.8470 - lr: 1.6000e-07\n",
      "Epoch 75/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.2952 - auc: 0.9604 - acc: 0.8912 - positive: 2307.0000 - negative: 2336.0000 - precision: 0.8956 - recall: 0.8856\n",
      "Epoch 75: val_loss did not improve from 0.42846\n",
      "82/82 [==============================] - 193s 2s/step - loss: 0.2952 - auc: 0.9604 - acc: 0.8912 - positive: 2307.0000 - negative: 2336.0000 - precision: 0.8956 - recall: 0.8856 - val_loss: 0.5465 - val_auc: 0.9190 - val_acc: 0.8405 - val_positive: 513.0000 - val_negative: 509.0000 - val_precision: 0.8382 - val_recall: 0.8438 - lr: 1.6000e-07\n",
      "Epoch 76/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.2832 - auc: 0.9618 - acc: 0.8946 - positive: 2326.0000 - negative: 2335.0000 - precision: 0.8960 - recall: 0.8929\n",
      "Epoch 76: val_loss did not improve from 0.42846\n",
      "82/82 [==============================] - 194s 2s/step - loss: 0.2832 - auc: 0.9618 - acc: 0.8946 - positive: 2326.0000 - negative: 2335.0000 - precision: 0.8960 - recall: 0.8929 - val_loss: 0.4942 - val_auc: 0.9293 - val_acc: 0.8577 - val_positive: 526.0000 - val_negative: 517.0000 - val_precision: 0.8525 - val_recall: 0.8651 - lr: 1.6000e-07\n",
      "Epoch 77/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.2961 - auc: 0.9591 - acc: 0.8879 - positive: 2322.0000 - negative: 2304.0000 - precision: 0.8852 - recall: 0.8914\n",
      "Epoch 77: val_loss did not improve from 0.42846\n",
      "82/82 [==============================] - 193s 2s/step - loss: 0.2961 - auc: 0.9591 - acc: 0.8879 - positive: 2322.0000 - negative: 2304.0000 - precision: 0.8852 - recall: 0.8914 - val_loss: 0.5116 - val_auc: 0.9129 - val_acc: 0.8248 - val_positive: 497.0000 - val_negative: 506.0000 - val_precision: 0.8297 - val_recall: 0.8174 - lr: 1.6000e-07\n",
      "Epoch 78/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.2863 - auc: 0.9607 - acc: 0.8879 - positive: 2308.0000 - negative: 2318.0000 - precision: 0.8894 - recall: 0.8860\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 3.199999980552093e-08.\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.42846\n",
      "82/82 [==============================] - 195s 2s/step - loss: 0.2863 - auc: 0.9607 - acc: 0.8879 - positive: 2308.0000 - negative: 2318.0000 - precision: 0.8894 - recall: 0.8860 - val_loss: 0.5849 - val_auc: 0.9094 - val_acc: 0.8331 - val_positive: 510.0000 - val_negative: 503.0000 - val_precision: 0.8293 - val_recall: 0.8388 - lr: 1.6000e-07\n",
      "Epoch 79/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.2954 - auc: 0.9592 - acc: 0.8919 - positive: 2322.0000 - negative: 2325.0000 - precision: 0.8924 - recall: 0.8914\n",
      "Epoch 79: val_loss improved from 0.42846 to 0.42558, saving model to best_model.h5\n",
      "82/82 [==============================] - 193s 2s/step - loss: 0.2954 - auc: 0.9592 - acc: 0.8919 - positive: 2322.0000 - negative: 2325.0000 - precision: 0.8924 - recall: 0.8914 - val_loss: 0.4256 - val_auc: 0.9383 - val_acc: 0.8766 - val_positive: 530.0000 - val_negative: 536.0000 - val_precision: 0.8804 - val_recall: 0.8717 - lr: 3.2000e-08\n",
      "Epoch 80/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.2985 - auc: 0.9591 - acc: 0.8919 - positive: 2327.0000 - negative: 2320.0000 - precision: 0.8909 - recall: 0.8933\n",
      "Epoch 80: val_loss did not improve from 0.42558\n",
      "82/82 [==============================] - 194s 2s/step - loss: 0.2985 - auc: 0.9591 - acc: 0.8919 - positive: 2327.0000 - negative: 2320.0000 - precision: 0.8909 - recall: 0.8933 - val_loss: 0.4847 - val_auc: 0.9280 - val_acc: 0.8479 - val_positive: 512.0000 - val_negative: 519.0000 - val_precision: 0.8519 - val_recall: 0.8421 - lr: 3.2000e-08\n",
      "Epoch 81/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.2881 - auc: 0.9612 - acc: 0.8944 - positive: 2322.0000 - negative: 2338.0000 - precision: 0.8969 - recall: 0.8914\n",
      "Epoch 81: val_loss did not improve from 0.42558\n",
      "82/82 [==============================] - 193s 2s/step - loss: 0.2881 - auc: 0.9612 - acc: 0.8944 - positive: 2322.0000 - negative: 2338.0000 - precision: 0.8969 - recall: 0.8914 - val_loss: 0.4424 - val_auc: 0.9326 - val_acc: 0.8536 - val_positive: 521.0000 - val_negative: 517.0000 - val_precision: 0.8513 - val_recall: 0.8569 - lr: 3.2000e-08\n",
      "Epoch 82/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.2986 - auc: 0.9583 - acc: 0.8862 - positive: 2316.0000 - negative: 2301.0000 - precision: 0.8840 - recall: 0.8891\n",
      "Epoch 82: val_loss did not improve from 0.42558\n",
      "82/82 [==============================] - 196s 2s/step - loss: 0.2986 - auc: 0.9583 - acc: 0.8862 - positive: 2316.0000 - negative: 2301.0000 - precision: 0.8840 - recall: 0.8891 - val_loss: 0.5778 - val_auc: 0.9122 - val_acc: 0.8289 - val_positive: 500.0000 - val_negative: 508.0000 - val_precision: 0.8333 - val_recall: 0.8224 - lr: 3.2000e-08\n",
      "Epoch 83/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3039 - auc: 0.9575 - acc: 0.8856 - positive: 2302.0000 - negative: 2312.0000 - precision: 0.8871 - recall: 0.8837\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 6.399999818995639e-09.\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.42558\n",
      "82/82 [==============================] - 192s 2s/step - loss: 0.3039 - auc: 0.9575 - acc: 0.8856 - positive: 2302.0000 - negative: 2312.0000 - precision: 0.8871 - recall: 0.8837 - val_loss: 0.5058 - val_auc: 0.9193 - val_acc: 0.8421 - val_positive: 507.0000 - val_negative: 517.0000 - val_precision: 0.8478 - val_recall: 0.8339 - lr: 3.2000e-08\n",
      "Epoch 84/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.2927 - auc: 0.9605 - acc: 0.8910 - positive: 2330.0000 - negative: 2312.0000 - precision: 0.8883 - recall: 0.8944\n",
      "Epoch 84: val_loss did not improve from 0.42558\n",
      "82/82 [==============================] - 194s 2s/step - loss: 0.2927 - auc: 0.9605 - acc: 0.8910 - positive: 2330.0000 - negative: 2312.0000 - precision: 0.8883 - recall: 0.8944 - val_loss: 0.4985 - val_auc: 0.9215 - val_acc: 0.8512 - val_positive: 517.0000 - val_negative: 518.0000 - val_precision: 0.8517 - val_recall: 0.8503 - lr: 6.4000e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3106 - auc: 0.9566 - acc: 0.8858 - positive: 2320.0000 - negative: 2295.0000 - precision: 0.8821 - recall: 0.8906\n",
      "Epoch 85: val_loss did not improve from 0.42558\n",
      "82/82 [==============================] - 192s 2s/step - loss: 0.3106 - auc: 0.9566 - acc: 0.8858 - positive: 2320.0000 - negative: 2295.0000 - precision: 0.8821 - recall: 0.8906 - val_loss: 0.5061 - val_auc: 0.9236 - val_acc: 0.8470 - val_positive: 516.0000 - val_negative: 514.0000 - val_precision: 0.8459 - val_recall: 0.8487 - lr: 6.4000e-09\n",
      "Epoch 86/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3192 - auc: 0.9551 - acc: 0.8891 - positive: 2300.0000 - negative: 2332.0000 - precision: 0.8939 - recall: 0.8829\n",
      "Epoch 86: val_loss did not improve from 0.42558\n",
      "82/82 [==============================] - 194s 2s/step - loss: 0.3192 - auc: 0.9551 - acc: 0.8891 - positive: 2300.0000 - negative: 2332.0000 - precision: 0.8939 - recall: 0.8829 - val_loss: 0.4914 - val_auc: 0.9243 - val_acc: 0.8462 - val_positive: 514.0000 - val_negative: 515.0000 - val_precision: 0.8468 - val_recall: 0.8454 - lr: 6.4000e-09\n",
      "Epoch 87/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.2929 - auc: 0.9594 - acc: 0.8868 - positive: 2310.0000 - negative: 2310.0000 - precision: 0.8868 - recall: 0.8868\n",
      "Epoch 87: val_loss did not improve from 0.42558\n",
      "82/82 [==============================] - 192s 2s/step - loss: 0.2929 - auc: 0.9594 - acc: 0.8868 - positive: 2310.0000 - negative: 2310.0000 - precision: 0.8868 - recall: 0.8868 - val_loss: 0.4900 - val_auc: 0.9245 - val_acc: 0.8528 - val_positive: 524.0000 - val_negative: 513.0000 - val_precision: 0.8465 - val_recall: 0.8618 - lr: 6.4000e-09\n",
      "Epoch 88/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3083 - auc: 0.9569 - acc: 0.8835 - positive: 2309.0000 - negative: 2294.0000 - precision: 0.8813 - recall: 0.8864\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 1.279999928271991e-09.\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.42558\n",
      "82/82 [==============================] - 195s 2s/step - loss: 0.3083 - auc: 0.9569 - acc: 0.8835 - positive: 2309.0000 - negative: 2294.0000 - precision: 0.8813 - recall: 0.8864 - val_loss: 0.5409 - val_auc: 0.9181 - val_acc: 0.8569 - val_positive: 522.0000 - val_negative: 520.0000 - val_precision: 0.8557 - val_recall: 0.8586 - lr: 6.4000e-09\n",
      "Epoch 89/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.2982 - auc: 0.9588 - acc: 0.8881 - positive: 2320.0000 - negative: 2307.0000 - precision: 0.8862 - recall: 0.8906\n",
      "Epoch 89: val_loss did not improve from 0.42558\n",
      "82/82 [==============================] - 193s 2s/step - loss: 0.2982 - auc: 0.9588 - acc: 0.8881 - positive: 2320.0000 - negative: 2307.0000 - precision: 0.8862 - recall: 0.8906 - val_loss: 0.4423 - val_auc: 0.9314 - val_acc: 0.8586 - val_positive: 523.0000 - val_negative: 521.0000 - val_precision: 0.8574 - val_recall: 0.8602 - lr: 1.2800e-09\n",
      "Epoch 90/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.2898 - auc: 0.9608 - acc: 0.8877 - positive: 2302.0000 - negative: 2323.0000 - precision: 0.8909 - recall: 0.8837\n",
      "Epoch 90: val_loss did not improve from 0.42558\n",
      "82/82 [==============================] - 196s 2s/step - loss: 0.2898 - auc: 0.9608 - acc: 0.8877 - positive: 2302.0000 - negative: 2323.0000 - precision: 0.8909 - recall: 0.8837 - val_loss: 0.4749 - val_auc: 0.9285 - val_acc: 0.8487 - val_positive: 513.0000 - val_negative: 519.0000 - val_precision: 0.8522 - val_recall: 0.8438 - lr: 1.2800e-09\n",
      "Epoch 91/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.2998 - auc: 0.9581 - acc: 0.8839 - positive: 2309.0000 - negative: 2296.0000 - precision: 0.8820 - recall: 0.8864\n",
      "Epoch 91: val_loss did not improve from 0.42558\n",
      "82/82 [==============================] - 194s 2s/step - loss: 0.2998 - auc: 0.9581 - acc: 0.8839 - positive: 2309.0000 - negative: 2296.0000 - precision: 0.8820 - recall: 0.8864 - val_loss: 0.4798 - val_auc: 0.9235 - val_acc: 0.8487 - val_positive: 512.0000 - val_negative: 520.0000 - val_precision: 0.8533 - val_recall: 0.8421 - lr: 1.2800e-09\n",
      "Epoch 92/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.2742 - auc: 0.9634 - acc: 0.8969 - positive: 2343.0000 - negative: 2330.0000 - precision: 0.8950 - recall: 0.8994\n",
      "Epoch 92: val_loss did not improve from 0.42558\n",
      "82/82 [==============================] - 195s 2s/step - loss: 0.2742 - auc: 0.9634 - acc: 0.8969 - positive: 2343.0000 - negative: 2330.0000 - precision: 0.8950 - recall: 0.8994 - val_loss: 0.5660 - val_auc: 0.9102 - val_acc: 0.8240 - val_positive: 500.0000 - val_negative: 502.0000 - val_precision: 0.8251 - val_recall: 0.8224 - lr: 1.2800e-09\n",
      "Epoch 93/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3123 - auc: 0.9560 - acc: 0.8898 - positive: 2313.0000 - negative: 2323.0000 - precision: 0.8913 - recall: 0.8879\n",
      "Epoch 93: val_loss improved from 0.42558 to 0.42249, saving model to best_model.h5\n",
      "82/82 [==============================] - 195s 2s/step - loss: 0.3123 - auc: 0.9560 - acc: 0.8898 - positive: 2313.0000 - negative: 2323.0000 - precision: 0.8913 - recall: 0.8879 - val_loss: 0.4225 - val_auc: 0.9320 - val_acc: 0.8602 - val_positive: 526.0000 - val_negative: 520.0000 - val_precision: 0.8567 - val_recall: 0.8651 - lr: 1.2800e-09\n",
      "Epoch 94/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.2927 - auc: 0.9601 - acc: 0.8937 - positive: 2326.0000 - negative: 2330.0000 - precision: 0.8943 - recall: 0.8929\n",
      "Epoch 94: val_loss did not improve from 0.42249\n",
      "82/82 [==============================] - 195s 2s/step - loss: 0.2927 - auc: 0.9601 - acc: 0.8937 - positive: 2326.0000 - negative: 2330.0000 - precision: 0.8943 - recall: 0.8929 - val_loss: 0.5234 - val_auc: 0.9153 - val_acc: 0.8322 - val_positive: 503.0000 - val_negative: 509.0000 - val_precision: 0.8355 - val_recall: 0.8273 - lr: 1.2800e-09\n",
      "Epoch 95/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3071 - auc: 0.9571 - acc: 0.8898 - positive: 2337.0000 - negative: 2299.0000 - precision: 0.8842 - recall: 0.8971\n",
      "Epoch 95: val_loss did not improve from 0.42249\n",
      "82/82 [==============================] - 194s 2s/step - loss: 0.3071 - auc: 0.9571 - acc: 0.8898 - positive: 2337.0000 - negative: 2299.0000 - precision: 0.8842 - recall: 0.8971 - val_loss: 0.4347 - val_auc: 0.9360 - val_acc: 0.8618 - val_positive: 519.0000 - val_negative: 529.0000 - val_precision: 0.8679 - val_recall: 0.8536 - lr: 1.2800e-09\n",
      "Epoch 96/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3169 - auc: 0.9557 - acc: 0.8860 - positive: 2300.0000 - negative: 2316.0000 - precision: 0.8884 - recall: 0.8829\n",
      "Epoch 96: val_loss did not improve from 0.42249\n",
      "82/82 [==============================] - 195s 2s/step - loss: 0.3169 - auc: 0.9557 - acc: 0.8860 - positive: 2300.0000 - negative: 2316.0000 - precision: 0.8884 - recall: 0.8829 - val_loss: 0.4568 - val_auc: 0.9303 - val_acc: 0.8487 - val_positive: 517.0000 - val_negative: 515.0000 - val_precision: 0.8475 - val_recall: 0.8503 - lr: 1.2800e-09\n",
      "Epoch 97/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.2807 - auc: 0.9623 - acc: 0.8985 - positive: 2348.0000 - negative: 2333.0000 - precision: 0.8962 - recall: 0.9013\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 2.55999976772614e-10.\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.42249\n",
      "82/82 [==============================] - 195s 2s/step - loss: 0.2807 - auc: 0.9623 - acc: 0.8985 - positive: 2348.0000 - negative: 2333.0000 - precision: 0.8962 - recall: 0.9013 - val_loss: 0.4974 - val_auc: 0.9202 - val_acc: 0.8405 - val_positive: 513.0000 - val_negative: 509.0000 - val_precision: 0.8382 - val_recall: 0.8438 - lr: 1.2800e-09\n",
      "Epoch 98/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.2791 - auc: 0.9619 - acc: 0.8948 - positive: 2344.0000 - negative: 2318.0000 - precision: 0.8909 - recall: 0.8998\n",
      "Epoch 98: val_loss did not improve from 0.42249\n",
      "82/82 [==============================] - 194s 2s/step - loss: 0.2791 - auc: 0.9619 - acc: 0.8948 - positive: 2344.0000 - negative: 2318.0000 - precision: 0.8909 - recall: 0.8998 - val_loss: 0.4742 - val_auc: 0.9247 - val_acc: 0.8479 - val_positive: 521.0000 - val_negative: 510.0000 - val_precision: 0.8417 - val_recall: 0.8569 - lr: 2.5600e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.2955 - auc: 0.9603 - acc: 0.8950 - positive: 2332.0000 - negative: 2331.0000 - precision: 0.8949 - recall: 0.8952\n",
      "Epoch 99: val_loss did not improve from 0.42249\n",
      "82/82 [==============================] - 195s 2s/step - loss: 0.2955 - auc: 0.9603 - acc: 0.8950 - positive: 2332.0000 - negative: 2331.0000 - precision: 0.8949 - recall: 0.8952 - val_loss: 0.4961 - val_auc: 0.9236 - val_acc: 0.8495 - val_positive: 516.0000 - val_negative: 517.0000 - val_precision: 0.8501 - val_recall: 0.8487 - lr: 2.5600e-10\n",
      "Epoch 100/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3161 - auc: 0.9551 - acc: 0.8866 - positive: 2320.0000 - negative: 2299.0000 - precision: 0.8835 - recall: 0.8906\n",
      "Epoch 100: val_loss did not improve from 0.42249\n",
      "82/82 [==============================] - 193s 2s/step - loss: 0.3161 - auc: 0.9551 - acc: 0.8866 - positive: 2320.0000 - negative: 2299.0000 - precision: 0.8835 - recall: 0.8906 - val_loss: 0.4982 - val_auc: 0.9238 - val_acc: 0.8438 - val_positive: 511.0000 - val_negative: 515.0000 - val_precision: 0.8460 - val_recall: 0.8405 - lr: 2.5600e-10\n",
      "Training completed in time:   5:22:05.648275\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "model_history=model.fit_generator(\n",
    "                            train_set,\n",
    "                            validation_data=test_set,\n",
    "                            epochs=100,\n",
    "                            steps_per_epoch= 2637 // train_set.batch_size,\n",
    "                            validation_steps=630 // test_set.batch_size,\n",
    "                            validation_freq=1,\n",
    "                            callbacks=callbacks, \n",
    "                            verbose=1)\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time:  \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b726366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'auc', 'acc', 'positive', 'negative', 'precision', 'recall', 'val_loss', 'val_auc', 'val_acc', 'val_positive', 'val_negative', 'val_precision', 'val_recall', 'lr'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7de61925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x25ac3287208>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title('Losses comparison ')\n",
    "plt.plot(model_history.history.get('loss'), label='loss')\n",
    "plt.plot(model_history.history.get('val_loss'), label='val_loss')\n",
    "plt.xlabel('Loss')\n",
    "plt.ylabel('Epoch')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70e5edf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [1.2742924690246582,\n",
       "  0.9122977256774902,\n",
       "  0.8220303058624268,\n",
       "  0.7550947070121765,\n",
       "  0.7227153778076172,\n",
       "  0.6434211134910583,\n",
       "  0.6406369805335999,\n",
       "  0.6021507978439331,\n",
       "  0.5797320604324341,\n",
       "  0.5522443652153015,\n",
       "  0.5489218831062317,\n",
       "  0.5411645770072937,\n",
       "  0.5130136609077454,\n",
       "  0.4948704242706299,\n",
       "  0.5034021735191345,\n",
       "  0.47689029574394226,\n",
       "  0.49464133381843567,\n",
       "  0.47425299882888794,\n",
       "  0.47696515917778015,\n",
       "  0.4350612759590149,\n",
       "  0.44473373889923096,\n",
       "  0.4671701490879059,\n",
       "  0.424052357673645,\n",
       "  0.4205182194709778,\n",
       "  0.41313162446022034,\n",
       "  0.438821017742157,\n",
       "  0.4160769283771515,\n",
       "  0.42159464955329895,\n",
       "  0.4019327759742737,\n",
       "  0.41128379106521606,\n",
       "  0.3906456530094147,\n",
       "  0.4001179039478302,\n",
       "  0.3899914622306824,\n",
       "  0.4071555733680725,\n",
       "  0.3580264449119568,\n",
       "  0.3926863670349121,\n",
       "  0.3648426830768585,\n",
       "  0.384025901556015,\n",
       "  0.3856808841228485,\n",
       "  0.37172532081604004,\n",
       "  0.3394094407558441,\n",
       "  0.34682947397232056,\n",
       "  0.3348223865032196,\n",
       "  0.32663074135780334,\n",
       "  0.33128026127815247,\n",
       "  0.33071544766426086,\n",
       "  0.33008912205696106,\n",
       "  0.3363886773586273,\n",
       "  0.30878108739852905,\n",
       "  0.3073064088821411,\n",
       "  0.31633853912353516,\n",
       "  0.3179660141468048,\n",
       "  0.2998892068862915,\n",
       "  0.2974037826061249,\n",
       "  0.33093398809432983,\n",
       "  0.2959044873714447,\n",
       "  0.29852116107940674,\n",
       "  0.308224081993103,\n",
       "  0.30579864978790283,\n",
       "  0.31179848313331604,\n",
       "  0.28573450446128845,\n",
       "  0.324699729681015,\n",
       "  0.32532036304473877,\n",
       "  0.300971120595932,\n",
       "  0.2884208858013153,\n",
       "  0.2939196228981018,\n",
       "  0.28620055317878723,\n",
       "  0.28862565755844116,\n",
       "  0.2951117157936096,\n",
       "  0.3103466331958771,\n",
       "  0.2970412075519562,\n",
       "  0.2780901789665222,\n",
       "  0.2817401885986328,\n",
       "  0.3060497045516968,\n",
       "  0.29521000385284424,\n",
       "  0.28324130177497864,\n",
       "  0.29612061381340027,\n",
       "  0.286276638507843,\n",
       "  0.29540079832077026,\n",
       "  0.29846465587615967,\n",
       "  0.28812068700790405,\n",
       "  0.2986268699169159,\n",
       "  0.3039237856864929,\n",
       "  0.2927444875240326,\n",
       "  0.3105992376804352,\n",
       "  0.3192480206489563,\n",
       "  0.2929180860519409,\n",
       "  0.30826759338378906,\n",
       "  0.29819798469543457,\n",
       "  0.28983014822006226,\n",
       "  0.29979589581489563,\n",
       "  0.2741890549659729,\n",
       "  0.31229254603385925,\n",
       "  0.29268720746040344,\n",
       "  0.3071451187133789,\n",
       "  0.31689733266830444,\n",
       "  0.2807464301586151,\n",
       "  0.27906134724617004,\n",
       "  0.29550936818122864,\n",
       "  0.3161398470401764],\n",
       " 'auc': [0.7763993740081787,\n",
       "  0.8559182286262512,\n",
       "  0.8653026819229126,\n",
       "  0.8809676766395569,\n",
       "  0.8831126093864441,\n",
       "  0.8971325755119324,\n",
       "  0.8985223174095154,\n",
       "  0.9073609709739685,\n",
       "  0.9115800857543945,\n",
       "  0.9139283299446106,\n",
       "  0.9120910167694092,\n",
       "  0.9155893921852112,\n",
       "  0.9225923418998718,\n",
       "  0.9234712719917297,\n",
       "  0.9230474233627319,\n",
       "  0.9287829399108887,\n",
       "  0.9252921938896179,\n",
       "  0.9302034974098206,\n",
       "  0.9278191924095154,\n",
       "  0.9350994825363159,\n",
       "  0.9334298372268677,\n",
       "  0.9314876198768616,\n",
       "  0.9376382827758789,\n",
       "  0.9361292719841003,\n",
       "  0.9404438138008118,\n",
       "  0.9345696568489075,\n",
       "  0.9396798610687256,\n",
       "  0.9380292892456055,\n",
       "  0.9416966438293457,\n",
       "  0.9392740726470947,\n",
       "  0.942466676235199,\n",
       "  0.9410245418548584,\n",
       "  0.9432295560836792,\n",
       "  0.9411973357200623,\n",
       "  0.949701189994812,\n",
       "  0.9432427287101746,\n",
       "  0.9490351676940918,\n",
       "  0.9444753527641296,\n",
       "  0.9440300464630127,\n",
       "  0.9459667801856995,\n",
       "  0.9513078331947327,\n",
       "  0.9505295157432556,\n",
       "  0.9524240493774414,\n",
       "  0.9543523788452148,\n",
       "  0.9531655311584473,\n",
       "  0.95450758934021,\n",
       "  0.9539937376976013,\n",
       "  0.9537741541862488,\n",
       "  0.9572460055351257,\n",
       "  0.9578454494476318,\n",
       "  0.9563624858856201,\n",
       "  0.9554426670074463,\n",
       "  0.9574708342552185,\n",
       "  0.9594942331314087,\n",
       "  0.9526600241661072,\n",
       "  0.9592199921607971,\n",
       "  0.958795964717865,\n",
       "  0.9567651152610779,\n",
       "  0.9571395516395569,\n",
       "  0.9567434191703796,\n",
       "  0.9609569311141968,\n",
       "  0.9546028971672058,\n",
       "  0.9536193609237671,\n",
       "  0.9580286741256714,\n",
       "  0.96092289686203,\n",
       "  0.9595372080802917,\n",
       "  0.9607768058776855,\n",
       "  0.9600979089736938,\n",
       "  0.9597512483596802,\n",
       "  0.9570037126541138,\n",
       "  0.9590436816215515,\n",
       "  0.9615899324417114,\n",
       "  0.9626420736312866,\n",
       "  0.9575663208961487,\n",
       "  0.9604446887969971,\n",
       "  0.9617646932601929,\n",
       "  0.9590668678283691,\n",
       "  0.9607123136520386,\n",
       "  0.9591662883758545,\n",
       "  0.9591423273086548,\n",
       "  0.9611735343933105,\n",
       "  0.958300769329071,\n",
       "  0.9575418829917908,\n",
       "  0.9604517817497253,\n",
       "  0.9566326141357422,\n",
       "  0.9551160931587219,\n",
       "  0.9593532681465149,\n",
       "  0.9569498896598816,\n",
       "  0.9588402509689331,\n",
       "  0.9608394503593445,\n",
       "  0.9580873250961304,\n",
       "  0.9633595943450928,\n",
       "  0.9559833407402039,\n",
       "  0.9601477384567261,\n",
       "  0.9570815563201904,\n",
       "  0.9556708931922913,\n",
       "  0.9623242020606995,\n",
       "  0.9619373083114624,\n",
       "  0.96025550365448,\n",
       "  0.9550523161888123],\n",
       " 'acc': [0.7151631712913513,\n",
       "  0.7867562174797058,\n",
       "  0.7894433736801147,\n",
       "  0.8147792816162109,\n",
       "  0.8092130422592163,\n",
       "  0.8245681524276733,\n",
       "  0.8266794681549072,\n",
       "  0.8330134153366089,\n",
       "  0.8347408771514893,\n",
       "  0.8397312760353088,\n",
       "  0.8378118872642517,\n",
       "  0.8358924984931946,\n",
       "  0.8543186187744141,\n",
       "  0.8472169041633606,\n",
       "  0.8508636951446533,\n",
       "  0.8508636951446533,\n",
       "  0.848944365978241,\n",
       "  0.8558541536331177,\n",
       "  0.8539347648620605,\n",
       "  0.8629558682441711,\n",
       "  0.8579654693603516,\n",
       "  0.8595009446144104,\n",
       "  0.8692898154258728,\n",
       "  0.8631477952003479,\n",
       "  0.8667946457862854,\n",
       "  0.8595009446144104,\n",
       "  0.8618042469024658,\n",
       "  0.86449134349823,\n",
       "  0.8723608255386353,\n",
       "  0.8642994165420532,\n",
       "  0.8746641278266907,\n",
       "  0.868522047996521,\n",
       "  0.8715931177139282,\n",
       "  0.8704414367675781,\n",
       "  0.8815739154815674,\n",
       "  0.8723608255386353,\n",
       "  0.8811900019645691,\n",
       "  0.8761996030807495,\n",
       "  0.8746641278266907,\n",
       "  0.8744721412658691,\n",
       "  0.8840690851211548,\n",
       "  0.8765835165977478,\n",
       "  0.8871400952339172,\n",
       "  0.8890594840049744,\n",
       "  0.8788867592811584,\n",
       "  0.8803353905677795,\n",
       "  0.8834933042526245,\n",
       "  0.8848368525505066,\n",
       "  0.8919385671615601,\n",
       "  0.8873320817947388,\n",
       "  0.8859884738922119,\n",
       "  0.8769673705101013,\n",
       "  0.8892514109611511,\n",
       "  0.8884837031364441,\n",
       "  0.8794625997543335,\n",
       "  0.8902438879013062,\n",
       "  0.8948176503181458,\n",
       "  0.882533609867096,\n",
       "  0.8859884738922119,\n",
       "  0.886564314365387,\n",
       "  0.8917466402053833,\n",
       "  0.8844529986381531,\n",
       "  0.8884837031364441,\n",
       "  0.8892514109611511,\n",
       "  0.8907869458198547,\n",
       "  0.8873320817947388,\n",
       "  0.8961611986160278,\n",
       "  0.8911707997322083,\n",
       "  0.8902111053466797,\n",
       "  0.8896353244781494,\n",
       "  0.8913627862930298,\n",
       "  0.890595018863678,\n",
       "  0.8936660289764404,\n",
       "  0.8880997896194458,\n",
       "  0.8911707997322083,\n",
       "  0.894625723361969,\n",
       "  0.887907862663269,\n",
       "  0.887907862663269,\n",
       "  0.8919385671615601,\n",
       "  0.8919385671615601,\n",
       "  0.8944337964057922,\n",
       "  0.8861804008483887,\n",
       "  0.8856046199798584,\n",
       "  0.8909788727760315,\n",
       "  0.8857965469360352,\n",
       "  0.8890594840049744,\n",
       "  0.8867562413215637,\n",
       "  0.8834933042526245,\n",
       "  0.8880997896194458,\n",
       "  0.8877159357070923,\n",
       "  0.883877158164978,\n",
       "  0.8969289660453796,\n",
       "  0.8898272514343262,\n",
       "  0.8936660289764404,\n",
       "  0.8898272514343262,\n",
       "  0.8859884738922119,\n",
       "  0.8984645009040833,\n",
       "  0.8948176503181458,\n",
       "  0.8950095772743225,\n",
       "  0.886564314365387],\n",
       " 'positive': [1864.0,\n",
       "  2055.0,\n",
       "  2058.0,\n",
       "  2117.0,\n",
       "  2109.0,\n",
       "  2149.0,\n",
       "  2144.0,\n",
       "  2174.0,\n",
       "  2174.0,\n",
       "  2179.0,\n",
       "  2181.0,\n",
       "  2167.0,\n",
       "  2231.0,\n",
       "  2208.0,\n",
       "  2207.0,\n",
       "  2228.0,\n",
       "  2207.0,\n",
       "  2228.0,\n",
       "  2229.0,\n",
       "  2244.0,\n",
       "  2236.0,\n",
       "  2245.0,\n",
       "  2266.0,\n",
       "  2242.0,\n",
       "  2261.0,\n",
       "  2235.0,\n",
       "  2241.0,\n",
       "  2254.0,\n",
       "  2273.0,\n",
       "  2245.0,\n",
       "  2270.0,\n",
       "  2267.0,\n",
       "  2262.0,\n",
       "  2277.0,\n",
       "  2288.0,\n",
       "  2259.0,\n",
       "  2287.0,\n",
       "  2279.0,\n",
       "  2280.0,\n",
       "  2270.0,\n",
       "  2307.0,\n",
       "  2281.0,\n",
       "  2311.0,\n",
       "  2326.0,\n",
       "  2295.0,\n",
       "  2309.0,\n",
       "  2296.0,\n",
       "  2295.0,\n",
       "  2319.0,\n",
       "  2316.0,\n",
       "  2314.0,\n",
       "  2281.0,\n",
       "  2319.0,\n",
       "  2315.0,\n",
       "  2296.0,\n",
       "  2332.0,\n",
       "  2332.0,\n",
       "  2297.0,\n",
       "  2313.0,\n",
       "  2319.0,\n",
       "  2323.0,\n",
       "  2307.0,\n",
       "  2307.0,\n",
       "  2313.0,\n",
       "  2327.0,\n",
       "  2306.0,\n",
       "  2330.0,\n",
       "  2326.0,\n",
       "  2335.0,\n",
       "  2292.0,\n",
       "  2328.0,\n",
       "  2312.0,\n",
       "  2343.0,\n",
       "  2319.0,\n",
       "  2307.0,\n",
       "  2326.0,\n",
       "  2322.0,\n",
       "  2308.0,\n",
       "  2322.0,\n",
       "  2327.0,\n",
       "  2322.0,\n",
       "  2316.0,\n",
       "  2302.0,\n",
       "  2330.0,\n",
       "  2320.0,\n",
       "  2300.0,\n",
       "  2310.0,\n",
       "  2309.0,\n",
       "  2320.0,\n",
       "  2302.0,\n",
       "  2309.0,\n",
       "  2343.0,\n",
       "  2313.0,\n",
       "  2326.0,\n",
       "  2337.0,\n",
       "  2300.0,\n",
       "  2348.0,\n",
       "  2344.0,\n",
       "  2332.0,\n",
       "  2320.0],\n",
       " 'negative': [1862.0,\n",
       "  2044.0,\n",
       "  2055.0,\n",
       "  2128.0,\n",
       "  2107.0,\n",
       "  2147.0,\n",
       "  2163.0,\n",
       "  2166.0,\n",
       "  2175.0,\n",
       "  2196.0,\n",
       "  2184.0,\n",
       "  2188.0,\n",
       "  2220.0,\n",
       "  2206.0,\n",
       "  2226.0,\n",
       "  2205.0,\n",
       "  2216.0,\n",
       "  2231.0,\n",
       "  2220.0,\n",
       "  2252.0,\n",
       "  2234.0,\n",
       "  2233.0,\n",
       "  2263.0,\n",
       "  2255.0,\n",
       "  2255.0,\n",
       "  2243.0,\n",
       "  2249.0,\n",
       "  2250.0,\n",
       "  2272.0,\n",
       "  2258.0,\n",
       "  2287.0,\n",
       "  2258.0,\n",
       "  2279.0,\n",
       "  2258.0,\n",
       "  2305.0,\n",
       "  2286.0,\n",
       "  2304.0,\n",
       "  2286.0,\n",
       "  2277.0,\n",
       "  2286.0,\n",
       "  2299.0,\n",
       "  2286.0,\n",
       "  2311.0,\n",
       "  2306.0,\n",
       "  2284.0,\n",
       "  2311.0,\n",
       "  2307.0,\n",
       "  2315.0,\n",
       "  2328.0,\n",
       "  2307.0,\n",
       "  2302.0,\n",
       "  2288.0,\n",
       "  2314.0,\n",
       "  2314.0,\n",
       "  2286.0,\n",
       "  2340.0,\n",
       "  2330.0,\n",
       "  2301.0,\n",
       "  2303.0,\n",
       "  2300.0,\n",
       "  2323.0,\n",
       "  2301.0,\n",
       "  2322.0,\n",
       "  2320.0,\n",
       "  2314.0,\n",
       "  2317.0,\n",
       "  2339.0,\n",
       "  2317.0,\n",
       "  2303.0,\n",
       "  2343.0,\n",
       "  2316.0,\n",
       "  2328.0,\n",
       "  2313.0,\n",
       "  2308.0,\n",
       "  2336.0,\n",
       "  2335.0,\n",
       "  2304.0,\n",
       "  2318.0,\n",
       "  2325.0,\n",
       "  2320.0,\n",
       "  2338.0,\n",
       "  2301.0,\n",
       "  2312.0,\n",
       "  2312.0,\n",
       "  2295.0,\n",
       "  2332.0,\n",
       "  2310.0,\n",
       "  2294.0,\n",
       "  2307.0,\n",
       "  2323.0,\n",
       "  2296.0,\n",
       "  2330.0,\n",
       "  2323.0,\n",
       "  2330.0,\n",
       "  2299.0,\n",
       "  2316.0,\n",
       "  2333.0,\n",
       "  2318.0,\n",
       "  2331.0,\n",
       "  2299.0],\n",
       " 'precision': [0.7149980664253235,\n",
       "  0.7855504751205444,\n",
       "  0.7891104221343994,\n",
       "  0.8161141276359558,\n",
       "  0.8089758157730103,\n",
       "  0.8243191242218018,\n",
       "  0.8290796875953674,\n",
       "  0.8319938778877258,\n",
       "  0.8348694443702698,\n",
       "  0.8419629335403442,\n",
       "  0.8382014036178589,\n",
       "  0.8386222720146179,\n",
       "  0.8528287410736084,\n",
       "  0.8469505310058594,\n",
       "  0.853441596031189,\n",
       "  0.8477929830551147,\n",
       "  0.8501541018486023,\n",
       "  0.8562644124031067,\n",
       "  0.8527161478996277,\n",
       "  0.8640739321708679,\n",
       "  0.8576908111572266,\n",
       "  0.8578525185585022,\n",
       "  0.8688650131225586,\n",
       "  0.8649691343307495,\n",
       "  0.8659517168998718,\n",
       "  0.8606083989143372,\n",
       "  0.8629187345504761,\n",
       "  0.8639325499534607,\n",
       "  0.8722179532051086,\n",
       "  0.866126537322998,\n",
       "  0.8771252036094666,\n",
       "  0.8672532439231873,\n",
       "  0.8740339875221252,\n",
       "  0.8677591681480408,\n",
       "  0.8840803503990173,\n",
       "  0.8762606382369995,\n",
       "  0.8836939930915833,\n",
       "  0.8772132396697998,\n",
       "  0.8742331266403198,\n",
       "  0.8767864108085632,\n",
       "  0.8828932046890259,\n",
       "  0.8773077130317688,\n",
       "  0.8871400952339172,\n",
       "  0.8860952258110046,\n",
       "  0.877293586730957,\n",
       "  0.8806254863739014,\n",
       "  0.8851194977760315,\n",
       "  0.8878143429756165,\n",
       "  0.8932973742485046,\n",
       "  0.8859984874725342,\n",
       "  0.8842185735702515,\n",
       "  0.8779830932617188,\n",
       "  0.888505756855011,\n",
       "  0.8883346319198608,\n",
       "  0.8780114650726318,\n",
       "  0.891437292098999,\n",
       "  0.8945147395133972,\n",
       "  0.8831218481063843,\n",
       "  0.8845124244689941,\n",
       "  0.8837652206420898,\n",
       "  0.8917466402053833,\n",
       "  0.8835695385932922,\n",
       "  0.8907335996627808,\n",
       "  0.8903002142906189,\n",
       "  0.8888464570045471,\n",
       "  0.8889745473861694,\n",
       "  0.8975346684455872,\n",
       "  0.8898240327835083,\n",
       "  0.8854759335517883,\n",
       "  0.897415816783905,\n",
       "  0.8895682096481323,\n",
       "  0.8930088877677917,\n",
       "  0.8891840577125549,\n",
       "  0.8864678740501404,\n",
       "  0.8955745100975037,\n",
       "  0.8959938287734985,\n",
       "  0.8852459192276001,\n",
       "  0.8894026875495911,\n",
       "  0.8923904895782471,\n",
       "  0.8908882141113281,\n",
       "  0.8968713879585266,\n",
       "  0.8839694857597351,\n",
       "  0.8870905637741089,\n",
       "  0.8882958292961121,\n",
       "  0.8821292519569397,\n",
       "  0.8938981890678406,\n",
       "  0.8867562413215637,\n",
       "  0.8812977075576782,\n",
       "  0.8861726522445679,\n",
       "  0.8908668756484985,\n",
       "  0.8819709420204163,\n",
       "  0.894957959651947,\n",
       "  0.8913294672966003,\n",
       "  0.8942714333534241,\n",
       "  0.8842224478721619,\n",
       "  0.8883739113807678,\n",
       "  0.89618319272995,\n",
       "  0.8909159898757935,\n",
       "  0.8948580026626587,\n",
       "  0.8834729790687561],\n",
       " 'recall': [0.7155470252037048,\n",
       "  0.7888675332069397,\n",
       "  0.7900192141532898,\n",
       "  0.812667965888977,\n",
       "  0.8095969557762146,\n",
       "  0.8249520063400269,\n",
       "  0.8230326175689697,\n",
       "  0.8345489501953125,\n",
       "  0.8345489501953125,\n",
       "  0.8364683389663696,\n",
       "  0.8372361063957214,\n",
       "  0.8318617939949036,\n",
       "  0.856429934501648,\n",
       "  0.8476007580757141,\n",
       "  0.8472169041633606,\n",
       "  0.8552783131599426,\n",
       "  0.8472169041633606,\n",
       "  0.8552783131599426,\n",
       "  0.8556621670722961,\n",
       "  0.8614203333854675,\n",
       "  0.8583493232727051,\n",
       "  0.8618042469024658,\n",
       "  0.8698656558990479,\n",
       "  0.8606525659561157,\n",
       "  0.8679462671279907,\n",
       "  0.8579654693603516,\n",
       "  0.8602687120437622,\n",
       "  0.8652591109275818,\n",
       "  0.8725528120994568,\n",
       "  0.8618042469024658,\n",
       "  0.8714011311531067,\n",
       "  0.8702495098114014,\n",
       "  0.8683301210403442,\n",
       "  0.8740882873535156,\n",
       "  0.8783109188079834,\n",
       "  0.8671784996986389,\n",
       "  0.8779270648956299,\n",
       "  0.8748560547828674,\n",
       "  0.875239908695221,\n",
       "  0.8714011311531067,\n",
       "  0.8856046199798584,\n",
       "  0.8756238222122192,\n",
       "  0.8871400952339172,\n",
       "  0.8928982615470886,\n",
       "  0.8809980750083923,\n",
       "  0.8799542784690857,\n",
       "  0.8813819289207458,\n",
       "  0.8809980750083923,\n",
       "  0.8902111053466797,\n",
       "  0.8890594840049744,\n",
       "  0.8882917761802673,\n",
       "  0.8756238222122192,\n",
       "  0.8902111053466797,\n",
       "  0.8886756300926208,\n",
       "  0.8813819289207458,\n",
       "  0.8887194991111755,\n",
       "  0.895201563835144,\n",
       "  0.8817658424377441,\n",
       "  0.887907862663269,\n",
       "  0.8902111053466797,\n",
       "  0.8917466402053833,\n",
       "  0.8856046199798584,\n",
       "  0.8856046199798584,\n",
       "  0.887907862663269,\n",
       "  0.8932821750640869,\n",
       "  0.8852207064628601,\n",
       "  0.8944337964057922,\n",
       "  0.8928982615470886,\n",
       "  0.8963531851768494,\n",
       "  0.879846453666687,\n",
       "  0.8936660289764404,\n",
       "  0.8875240087509155,\n",
       "  0.8994241952896118,\n",
       "  0.8902111053466797,\n",
       "  0.8856046199798584,\n",
       "  0.8928982615470886,\n",
       "  0.8913627862930298,\n",
       "  0.8859884738922119,\n",
       "  0.8913627862930298,\n",
       "  0.8932821750640869,\n",
       "  0.8913627862930298,\n",
       "  0.8890594840049744,\n",
       "  0.8836852312088013,\n",
       "  0.8944337964057922,\n",
       "  0.890595018863678,\n",
       "  0.8829174637794495,\n",
       "  0.8867562413215637,\n",
       "  0.8863723874092102,\n",
       "  0.890595018863678,\n",
       "  0.8836852312088013,\n",
       "  0.8863723874092102,\n",
       "  0.8994241952896118,\n",
       "  0.887907862663269,\n",
       "  0.8928982615470886,\n",
       "  0.8971208930015564,\n",
       "  0.8829174637794495,\n",
       "  0.901343584060669,\n",
       "  0.8998080492019653,\n",
       "  0.895201563835144,\n",
       "  0.890595018863678],\n",
       " 'val_loss': [0.9244803190231323,\n",
       "  0.8881074786186218,\n",
       "  0.8841614723205566,\n",
       "  0.8713784217834473,\n",
       "  0.7728716731071472,\n",
       "  0.7858715653419495,\n",
       "  0.7257162928581238,\n",
       "  0.6696484088897705,\n",
       "  0.7556219100952148,\n",
       "  0.7322810888290405,\n",
       "  0.7673925757408142,\n",
       "  0.7237774729728699,\n",
       "  0.709012508392334,\n",
       "  0.6101542711257935,\n",
       "  0.6862295269966125,\n",
       "  0.6379939913749695,\n",
       "  0.6102984547615051,\n",
       "  0.5808738470077515,\n",
       "  0.6647799015045166,\n",
       "  0.6260644793510437,\n",
       "  0.6015741229057312,\n",
       "  0.6516314148902893,\n",
       "  0.5449432730674744,\n",
       "  0.540931224822998,\n",
       "  0.5525643825531006,\n",
       "  0.5989444255828857,\n",
       "  0.6043727993965149,\n",
       "  0.5954046845436096,\n",
       "  0.5821366906166077,\n",
       "  0.6353515982627869,\n",
       "  0.5939414501190186,\n",
       "  0.6609996557235718,\n",
       "  0.5013206005096436,\n",
       "  0.5810032486915588,\n",
       "  0.5629753470420837,\n",
       "  0.5465319752693176,\n",
       "  0.5762867331504822,\n",
       "  0.5996711850166321,\n",
       "  0.5201345086097717,\n",
       "  0.6010822653770447,\n",
       "  0.5569740533828735,\n",
       "  0.5734677314758301,\n",
       "  0.5050089955329895,\n",
       "  0.5255424380302429,\n",
       "  0.5041711926460266,\n",
       "  0.4676418900489807,\n",
       "  0.5228073596954346,\n",
       "  0.48008522391319275,\n",
       "  0.483674556016922,\n",
       "  0.5289663076400757,\n",
       "  0.5210165977478027,\n",
       "  0.49938973784446716,\n",
       "  0.5509853959083557,\n",
       "  0.5527499914169312,\n",
       "  0.4407101571559906,\n",
       "  0.47278183698654175,\n",
       "  0.46855902671813965,\n",
       "  0.5285273194313049,\n",
       "  0.48908117413520813,\n",
       "  0.5428271293640137,\n",
       "  0.5346528887748718,\n",
       "  0.5375467538833618,\n",
       "  0.4553271234035492,\n",
       "  0.5675643682479858,\n",
       "  0.48237770795822144,\n",
       "  0.5821755528450012,\n",
       "  0.4517187774181366,\n",
       "  0.5054537653923035,\n",
       "  0.4765426814556122,\n",
       "  0.4990631341934204,\n",
       "  0.5041680932044983,\n",
       "  0.4970492422580719,\n",
       "  0.5141074657440186,\n",
       "  0.4284554421901703,\n",
       "  0.5464680790901184,\n",
       "  0.49423447251319885,\n",
       "  0.5116108059883118,\n",
       "  0.5848762392997742,\n",
       "  0.4255785048007965,\n",
       "  0.484749436378479,\n",
       "  0.44237473607063293,\n",
       "  0.5777994990348816,\n",
       "  0.5058267116546631,\n",
       "  0.4985397160053253,\n",
       "  0.5060862302780151,\n",
       "  0.49136990308761597,\n",
       "  0.49004927277565,\n",
       "  0.540912389755249,\n",
       "  0.4423010051250458,\n",
       "  0.4748901426792145,\n",
       "  0.4798436462879181,\n",
       "  0.5659696459770203,\n",
       "  0.42248842120170593,\n",
       "  0.5234075784683228,\n",
       "  0.4347038269042969,\n",
       "  0.45679062604904175,\n",
       "  0.4974217712879181,\n",
       "  0.47421741485595703,\n",
       "  0.49614062905311584,\n",
       "  0.4982199966907501],\n",
       " 'val_auc': [0.8567185401916504,\n",
       "  0.8508429527282715,\n",
       "  0.8661810159683228,\n",
       "  0.8640319108963013,\n",
       "  0.8740234375,\n",
       "  0.8682776689529419,\n",
       "  0.8933612704277039,\n",
       "  0.8933762311935425,\n",
       "  0.8768584132194519,\n",
       "  0.882798969745636,\n",
       "  0.8738733530044556,\n",
       "  0.8875791430473328,\n",
       "  0.8886150121688843,\n",
       "  0.9114980697631836,\n",
       "  0.8910916447639465,\n",
       "  0.8978084325790405,\n",
       "  0.9120201468467712,\n",
       "  0.9075364470481873,\n",
       "  0.9034556746482849,\n",
       "  0.9028563499450684,\n",
       "  0.9087509512901306,\n",
       "  0.8977381587028503,\n",
       "  0.9153622984886169,\n",
       "  0.9106675386428833,\n",
       "  0.9142640829086304,\n",
       "  0.9093719124794006,\n",
       "  0.9033283591270447,\n",
       "  0.9111598134040833,\n",
       "  0.9090849757194519,\n",
       "  0.899453341960907,\n",
       "  0.9103036522865295,\n",
       "  0.9056399464607239,\n",
       "  0.9230029582977295,\n",
       "  0.9107933640480042,\n",
       "  0.9182338714599609,\n",
       "  0.9211986660957336,\n",
       "  0.9112937450408936,\n",
       "  0.919038712978363,\n",
       "  0.9210838675498962,\n",
       "  0.9015511274337769,\n",
       "  0.9127330183982849,\n",
       "  0.9147982597351074,\n",
       "  0.9234088659286499,\n",
       "  0.9211406111717224,\n",
       "  0.9260625839233398,\n",
       "  0.9322208762168884,\n",
       "  0.9226851463317871,\n",
       "  0.9258853197097778,\n",
       "  0.9294196963310242,\n",
       "  0.9189549684524536,\n",
       "  0.9173046946525574,\n",
       "  0.9253336191177368,\n",
       "  0.9122500419616699,\n",
       "  0.9134321212768555,\n",
       "  0.9323791861534119,\n",
       "  0.9286555647850037,\n",
       "  0.930720865726471,\n",
       "  0.9149417281150818,\n",
       "  0.9243162870407104,\n",
       "  0.9162212610244751,\n",
       "  0.918705940246582,\n",
       "  0.9127978086471558,\n",
       "  0.9286028146743774,\n",
       "  0.9111084342002869,\n",
       "  0.92131507396698,\n",
       "  0.9080650806427002,\n",
       "  0.9321451187133789,\n",
       "  0.9293534159660339,\n",
       "  0.9276750683784485,\n",
       "  0.9180378317832947,\n",
       "  0.9191522598266602,\n",
       "  0.9240174889564514,\n",
       "  0.919043779373169,\n",
       "  0.935258686542511,\n",
       "  0.9189736247062683,\n",
       "  0.9292803406715393,\n",
       "  0.9129466414451599,\n",
       "  0.9094420671463013,\n",
       "  0.9383034706115723,\n",
       "  0.9279900789260864,\n",
       "  0.9326172471046448,\n",
       "  0.9121555685997009,\n",
       "  0.9193484783172607,\n",
       "  0.9214881658554077,\n",
       "  0.9236415028572083,\n",
       "  0.9243177771568298,\n",
       "  0.9244584441184998,\n",
       "  0.9180553555488586,\n",
       "  0.9313592314720154,\n",
       "  0.9284728169441223,\n",
       "  0.9234737157821655,\n",
       "  0.9101521372795105,\n",
       "  0.9319883584976196,\n",
       "  0.9152960777282715,\n",
       "  0.9359933137893677,\n",
       "  0.9302636981010437,\n",
       "  0.9201869964599609,\n",
       "  0.9246654510498047,\n",
       "  0.9235548377037048,\n",
       "  0.9237712025642395],\n",
       " 'val_acc': [0.7861841917037964,\n",
       "  0.7861841917037964,\n",
       "  0.8026315569877625,\n",
       "  0.7878289222717285,\n",
       "  0.8009868264198303,\n",
       "  0.7935855388641357,\n",
       "  0.8231908082962036,\n",
       "  0.8108552694320679,\n",
       "  0.8075658082962036,\n",
       "  0.8050987124443054,\n",
       "  0.7985197305679321,\n",
       "  0.8223684430122375,\n",
       "  0.8182565569877625,\n",
       "  0.8363487124443054,\n",
       "  0.7993420958518982,\n",
       "  0.8141447305679321,\n",
       "  0.8363487124443054,\n",
       "  0.8174341917037964,\n",
       "  0.8256579041481018,\n",
       "  0.8256579041481018,\n",
       "  0.828125,\n",
       "  0.8199012875556946,\n",
       "  0.8330591917037964,\n",
       "  0.8404605388641357,\n",
       "  0.8314144611358643,\n",
       "  0.8264802694320679,\n",
       "  0.8322368264198303,\n",
       "  0.8264802694320679,\n",
       "  0.8240131735801697,\n",
       "  0.8248355388641357,\n",
       "  0.8322368264198303,\n",
       "  0.8338815569877625,\n",
       "  0.8429276347160339,\n",
       "  0.8363487124443054,\n",
       "  0.8355262875556946,\n",
       "  0.8519737124443054,\n",
       "  0.8305920958518982,\n",
       "  0.8478618264198303,\n",
       "  0.8470394611358643,\n",
       "  0.8322368264198303,\n",
       "  0.8412829041481018,\n",
       "  0.8338815569877625,\n",
       "  0.8412829041481018,\n",
       "  0.84375,\n",
       "  0.8626644611358643,\n",
       "  0.8577302694320679,\n",
       "  0.8486841917037964,\n",
       "  0.8445723652839661,\n",
       "  0.8519737124443054,\n",
       "  0.8462170958518982,\n",
       "  0.8396381735801697,\n",
       "  0.8527960777282715,\n",
       "  0.8297697305679321,\n",
       "  0.8363487124443054,\n",
       "  0.8462170958518982,\n",
       "  0.8601973652839661,\n",
       "  0.8544408082962036,\n",
       "  0.8404605388641357,\n",
       "  0.8305920958518982,\n",
       "  0.8396381735801697,\n",
       "  0.8470394611358643,\n",
       "  0.8379934430122375,\n",
       "  0.8470394611358643,\n",
       "  0.8338815569877625,\n",
       "  0.8495065569877625,\n",
       "  0.8273026347160339,\n",
       "  0.8511512875556946,\n",
       "  0.8453947305679321,\n",
       "  0.8544408082962036,\n",
       "  0.8338815569877625,\n",
       "  0.8396381735801697,\n",
       "  0.8421052694320679,\n",
       "  0.8429276347160339,\n",
       "  0.8544408082962036,\n",
       "  0.8404605388641357,\n",
       "  0.8577302694320679,\n",
       "  0.8248355388641357,\n",
       "  0.8330591917037964,\n",
       "  0.8766447305679321,\n",
       "  0.8478618264198303,\n",
       "  0.8536184430122375,\n",
       "  0.8289473652839661,\n",
       "  0.8421052694320679,\n",
       "  0.8511512875556946,\n",
       "  0.8470394611358643,\n",
       "  0.8462170958518982,\n",
       "  0.8527960777282715,\n",
       "  0.8569079041481018,\n",
       "  0.8585526347160339,\n",
       "  0.8486841917037964,\n",
       "  0.8486841917037964,\n",
       "  0.8240131735801697,\n",
       "  0.8601973652839661,\n",
       "  0.8322368264198303,\n",
       "  0.8618420958518982,\n",
       "  0.8486841917037964,\n",
       "  0.8404605388641357,\n",
       "  0.8478618264198303,\n",
       "  0.8495065569877625,\n",
       "  0.84375],\n",
       " 'val_positive': [478.0,\n",
       "  487.0,\n",
       "  476.0,\n",
       "  509.0,\n",
       "  488.0,\n",
       "  452.0,\n",
       "  478.0,\n",
       "  484.0,\n",
       "  470.0,\n",
       "  500.0,\n",
       "  491.0,\n",
       "  516.0,\n",
       "  489.0,\n",
       "  531.0,\n",
       "  485.0,\n",
       "  498.0,\n",
       "  505.0,\n",
       "  502.0,\n",
       "  492.0,\n",
       "  495.0,\n",
       "  511.0,\n",
       "  497.0,\n",
       "  514.0,\n",
       "  503.0,\n",
       "  489.0,\n",
       "  487.0,\n",
       "  503.0,\n",
       "  511.0,\n",
       "  504.0,\n",
       "  506.0,\n",
       "  511.0,\n",
       "  519.0,\n",
       "  515.0,\n",
       "  495.0,\n",
       "  517.0,\n",
       "  509.0,\n",
       "  494.0,\n",
       "  518.0,\n",
       "  499.0,\n",
       "  518.0,\n",
       "  509.0,\n",
       "  509.0,\n",
       "  515.0,\n",
       "  504.0,\n",
       "  528.0,\n",
       "  521.0,\n",
       "  523.0,\n",
       "  510.0,\n",
       "  509.0,\n",
       "  511.0,\n",
       "  506.0,\n",
       "  523.0,\n",
       "  503.0,\n",
       "  509.0,\n",
       "  501.0,\n",
       "  533.0,\n",
       "  524.0,\n",
       "  509.0,\n",
       "  509.0,\n",
       "  504.0,\n",
       "  521.0,\n",
       "  505.0,\n",
       "  516.0,\n",
       "  510.0,\n",
       "  518.0,\n",
       "  498.0,\n",
       "  520.0,\n",
       "  516.0,\n",
       "  517.0,\n",
       "  504.0,\n",
       "  510.0,\n",
       "  513.0,\n",
       "  515.0,\n",
       "  515.0,\n",
       "  513.0,\n",
       "  526.0,\n",
       "  497.0,\n",
       "  510.0,\n",
       "  530.0,\n",
       "  512.0,\n",
       "  521.0,\n",
       "  500.0,\n",
       "  507.0,\n",
       "  517.0,\n",
       "  516.0,\n",
       "  514.0,\n",
       "  524.0,\n",
       "  522.0,\n",
       "  523.0,\n",
       "  513.0,\n",
       "  512.0,\n",
       "  500.0,\n",
       "  526.0,\n",
       "  503.0,\n",
       "  519.0,\n",
       "  517.0,\n",
       "  513.0,\n",
       "  521.0,\n",
       "  516.0,\n",
       "  511.0],\n",
       " 'val_negative': [478.0,\n",
       "  469.0,\n",
       "  500.0,\n",
       "  449.0,\n",
       "  486.0,\n",
       "  513.0,\n",
       "  523.0,\n",
       "  502.0,\n",
       "  512.0,\n",
       "  479.0,\n",
       "  480.0,\n",
       "  484.0,\n",
       "  506.0,\n",
       "  486.0,\n",
       "  487.0,\n",
       "  492.0,\n",
       "  512.0,\n",
       "  492.0,\n",
       "  512.0,\n",
       "  509.0,\n",
       "  496.0,\n",
       "  500.0,\n",
       "  499.0,\n",
       "  519.0,\n",
       "  522.0,\n",
       "  518.0,\n",
       "  509.0,\n",
       "  494.0,\n",
       "  498.0,\n",
       "  497.0,\n",
       "  501.0,\n",
       "  495.0,\n",
       "  510.0,\n",
       "  522.0,\n",
       "  499.0,\n",
       "  527.0,\n",
       "  516.0,\n",
       "  513.0,\n",
       "  531.0,\n",
       "  494.0,\n",
       "  514.0,\n",
       "  505.0,\n",
       "  508.0,\n",
       "  522.0,\n",
       "  521.0,\n",
       "  522.0,\n",
       "  509.0,\n",
       "  517.0,\n",
       "  527.0,\n",
       "  518.0,\n",
       "  515.0,\n",
       "  514.0,\n",
       "  506.0,\n",
       "  508.0,\n",
       "  528.0,\n",
       "  513.0,\n",
       "  515.0,\n",
       "  513.0,\n",
       "  501.0,\n",
       "  517.0,\n",
       "  509.0,\n",
       "  514.0,\n",
       "  514.0,\n",
       "  504.0,\n",
       "  515.0,\n",
       "  508.0,\n",
       "  515.0,\n",
       "  512.0,\n",
       "  522.0,\n",
       "  510.0,\n",
       "  511.0,\n",
       "  511.0,\n",
       "  510.0,\n",
       "  524.0,\n",
       "  509.0,\n",
       "  517.0,\n",
       "  506.0,\n",
       "  503.0,\n",
       "  536.0,\n",
       "  519.0,\n",
       "  517.0,\n",
       "  508.0,\n",
       "  517.0,\n",
       "  518.0,\n",
       "  514.0,\n",
       "  515.0,\n",
       "  513.0,\n",
       "  520.0,\n",
       "  521.0,\n",
       "  519.0,\n",
       "  520.0,\n",
       "  502.0,\n",
       "  520.0,\n",
       "  509.0,\n",
       "  529.0,\n",
       "  515.0,\n",
       "  509.0,\n",
       "  510.0,\n",
       "  517.0,\n",
       "  515.0],\n",
       " 'val_precision': [0.7861841917037964,\n",
       "  0.7779552936553955,\n",
       "  0.8150684833526611,\n",
       "  0.7619760632514954,\n",
       "  0.800000011920929,\n",
       "  0.8263254165649414,\n",
       "  0.8490231037139893,\n",
       "  0.8203389644622803,\n",
       "  0.8303886651992798,\n",
       "  0.794912576675415,\n",
       "  0.7932148575782776,\n",
       "  0.8062499761581421,\n",
       "  0.8274111747741699,\n",
       "  0.8131699562072754,\n",
       "  0.8003300428390503,\n",
       "  0.8110749125480652,\n",
       "  0.840266227722168,\n",
       "  0.8122977614402771,\n",
       "  0.8367347121238708,\n",
       "  0.8333333134651184,\n",
       "  0.8202247023582458,\n",
       "  0.8214876055717468,\n",
       "  0.8250401020050049,\n",
       "  0.849662184715271,\n",
       "  0.8504347801208496,\n",
       "  0.8440207839012146,\n",
       "  0.8355481624603271,\n",
       "  0.8176000118255615,\n",
       "  0.8208469152450562,\n",
       "  0.8200972676277161,\n",
       "  0.8268608450889587,\n",
       "  0.8212025165557861,\n",
       "  0.8401305079460144,\n",
       "  0.8519793748855591,\n",
       "  0.8258786201477051,\n",
       "  0.8627118468284607,\n",
       "  0.8430033922195435,\n",
       "  0.8450244665145874,\n",
       "  0.8663194179534912,\n",
       "  0.8196202516555786,\n",
       "  0.844112753868103,\n",
       "  0.8316993713378906,\n",
       "  0.8373983502388,\n",
       "  0.8542372584342957,\n",
       "  0.8585366010665894,\n",
       "  0.8583195805549622,\n",
       "  0.8408359885215759,\n",
       "  0.8485856652259827,\n",
       "  0.8627118468284607,\n",
       "  0.8502495884895325,\n",
       "  0.8447412252426147,\n",
       "  0.8476499319076538,\n",
       "  0.831404983997345,\n",
       "  0.8357964158058167,\n",
       "  0.8623063564300537,\n",
       "  0.8487260937690735,\n",
       "  0.8492706418037415,\n",
       "  0.8427152037620544,\n",
       "  0.826298713684082,\n",
       "  0.8470588326454163,\n",
       "  0.8403225541114807,\n",
       "  0.8430717587471008,\n",
       "  0.8459016680717468,\n",
       "  0.8306189179420471,\n",
       "  0.8477904796600342,\n",
       "  0.832775890827179,\n",
       "  0.8482871055603027,\n",
       "  0.843137264251709,\n",
       "  0.8573797941207886,\n",
       "  0.8372092843055725,\n",
       "  0.8401976823806763,\n",
       "  0.8409836292266846,\n",
       "  0.8401305079460144,\n",
       "  0.8597663044929504,\n",
       "  0.8382353186607361,\n",
       "  0.8525121808052063,\n",
       "  0.8297162055969238,\n",
       "  0.8292682766914368,\n",
       "  0.880398690700531,\n",
       "  0.8519134521484375,\n",
       "  0.851307213306427,\n",
       "  0.8333333134651184,\n",
       "  0.8478260636329651,\n",
       "  0.8517298102378845,\n",
       "  0.8459016680717468,\n",
       "  0.8467874526977539,\n",
       "  0.8465266823768616,\n",
       "  0.8557376861572266,\n",
       "  0.8573770523071289,\n",
       "  0.8521594405174255,\n",
       "  0.8533333539962769,\n",
       "  0.8250824809074402,\n",
       "  0.8566775321960449,\n",
       "  0.8355481624603271,\n",
       "  0.8678929805755615,\n",
       "  0.8475409746170044,\n",
       "  0.8382353186607361,\n",
       "  0.841680109500885,\n",
       "  0.8500823974609375,\n",
       "  0.8460264801979065],\n",
       " 'val_recall': [0.7861841917037964,\n",
       "  0.8009868264198303,\n",
       "  0.7828947305679321,\n",
       "  0.8371710777282715,\n",
       "  0.8026315569877625,\n",
       "  0.7434210777282715,\n",
       "  0.7861841917037964,\n",
       "  0.7960526347160339,\n",
       "  0.7730262875556946,\n",
       "  0.8223684430122375,\n",
       "  0.8075658082962036,\n",
       "  0.8486841917037964,\n",
       "  0.8042762875556946,\n",
       "  0.8733552694320679,\n",
       "  0.7976973652839661,\n",
       "  0.8190789222717285,\n",
       "  0.8305920958518982,\n",
       "  0.8256579041481018,\n",
       "  0.8092105388641357,\n",
       "  0.8141447305679321,\n",
       "  0.8404605388641357,\n",
       "  0.8174341917037964,\n",
       "  0.8453947305679321,\n",
       "  0.8273026347160339,\n",
       "  0.8042762875556946,\n",
       "  0.8009868264198303,\n",
       "  0.8273026347160339,\n",
       "  0.8404605388641357,\n",
       "  0.8289473652839661,\n",
       "  0.8322368264198303,\n",
       "  0.8404605388641357,\n",
       "  0.8536184430122375,\n",
       "  0.8470394611358643,\n",
       "  0.8141447305679321,\n",
       "  0.8503289222717285,\n",
       "  0.8371710777282715,\n",
       "  0.8125,\n",
       "  0.8519737124443054,\n",
       "  0.8207237124443054,\n",
       "  0.8519737124443054,\n",
       "  0.8371710777282715,\n",
       "  0.8371710777282715,\n",
       "  0.8470394611358643,\n",
       "  0.8289473652839661,\n",
       "  0.8684210777282715,\n",
       "  0.8569079041481018,\n",
       "  0.8601973652839661,\n",
       "  0.8388158082962036,\n",
       "  0.8371710777282715,\n",
       "  0.8404605388641357,\n",
       "  0.8322368264198303,\n",
       "  0.8601973652839661,\n",
       "  0.8273026347160339,\n",
       "  0.8371710777282715,\n",
       "  0.8240131735801697,\n",
       "  0.8766447305679321,\n",
       "  0.8618420958518982,\n",
       "  0.8371710777282715,\n",
       "  0.8371710777282715,\n",
       "  0.8289473652839661,\n",
       "  0.8569079041481018,\n",
       "  0.8305920958518982,\n",
       "  0.8486841917037964,\n",
       "  0.8388158082962036,\n",
       "  0.8519737124443054,\n",
       "  0.8190789222717285,\n",
       "  0.8552631735801697,\n",
       "  0.8486841917037964,\n",
       "  0.8503289222717285,\n",
       "  0.8289473652839661,\n",
       "  0.8388158082962036,\n",
       "  0.84375,\n",
       "  0.8470394611358643,\n",
       "  0.8470394611358643,\n",
       "  0.84375,\n",
       "  0.8651315569877625,\n",
       "  0.8174341917037964,\n",
       "  0.8388158082962036,\n",
       "  0.8717105388641357,\n",
       "  0.8421052694320679,\n",
       "  0.8569079041481018,\n",
       "  0.8223684430122375,\n",
       "  0.8338815569877625,\n",
       "  0.8503289222717285,\n",
       "  0.8486841917037964,\n",
       "  0.8453947305679321,\n",
       "  0.8618420958518982,\n",
       "  0.8585526347160339,\n",
       "  0.8601973652839661,\n",
       "  0.84375,\n",
       "  0.8421052694320679,\n",
       "  0.8223684430122375,\n",
       "  0.8651315569877625,\n",
       "  0.8273026347160339,\n",
       "  0.8536184430122375,\n",
       "  0.8503289222717285,\n",
       "  0.84375,\n",
       "  0.8569079041481018,\n",
       "  0.8486841917037964,\n",
       "  0.8404605388641357],\n",
       " 'lr': [1e-04,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  2e-05,\n",
       "  2e-05,\n",
       "  2e-05,\n",
       "  2e-05,\n",
       "  2e-05,\n",
       "  2e-05,\n",
       "  2e-05,\n",
       "  2e-05,\n",
       "  2e-05,\n",
       "  2e-05,\n",
       "  2e-05,\n",
       "  2e-05,\n",
       "  2e-05,\n",
       "  2e-05,\n",
       "  2e-05,\n",
       "  2e-05,\n",
       "  2e-05,\n",
       "  2e-05,\n",
       "  2e-05,\n",
       "  4e-06,\n",
       "  4e-06,\n",
       "  4e-06,\n",
       "  4e-06,\n",
       "  4e-06,\n",
       "  4e-06,\n",
       "  4e-06,\n",
       "  8e-07,\n",
       "  8e-07,\n",
       "  8e-07,\n",
       "  8e-07,\n",
       "  8e-07,\n",
       "  1.6e-07,\n",
       "  1.6e-07,\n",
       "  1.6e-07,\n",
       "  1.6e-07,\n",
       "  1.6e-07,\n",
       "  1.6e-07,\n",
       "  1.6e-07,\n",
       "  3.2e-08,\n",
       "  3.2e-08,\n",
       "  3.2e-08,\n",
       "  3.2e-08,\n",
       "  3.2e-08,\n",
       "  6.3999996e-09,\n",
       "  6.3999996e-09,\n",
       "  6.3999996e-09,\n",
       "  6.3999996e-09,\n",
       "  6.3999996e-09,\n",
       "  1.2799999e-09,\n",
       "  1.2799999e-09,\n",
       "  1.2799999e-09,\n",
       "  1.2799999e-09,\n",
       "  1.2799999e-09,\n",
       "  1.2799999e-09,\n",
       "  1.2799999e-09,\n",
       "  1.2799999e-09,\n",
       "  1.2799999e-09,\n",
       "  2.5599997e-10,\n",
       "  2.5599997e-10,\n",
       "  2.5599997e-10]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbc0c19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
